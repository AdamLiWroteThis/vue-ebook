<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/><title>Myths and Facts About Static Application Security Testing Tools: An Action Research at Telenor Digital</title><link href="springer_epub.css" type="text/css" rel="styleSheet"/></head><body><div class="ChapterContextInformation"><div class="ContextInformation" id="Chap6"><div class="ChapterCopyright">© The Author(s) 2018</div><span class="ContextInformationAuthorEditorNames"><span class="Editor"><span class="EditorName">Juan Garbajosa</span>, </span><span class="Editor"><span class="EditorName">Xiaofeng Wang</span> and </span><span class="Editor"><span class="EditorName">Ademar Aguiar</span></span><span class="CollaboratorDesignation"> (eds.)</span></span><span class="ContextInformationBookTitles"><span class="BookTitle" xml:lang="en">Agile Processes in Software Engineering and Extreme Programming</span></span><span class="ContextInformationSeries"><span class="SeriesTitle" xml:lang="en">Lecture Notes in Business Information Processing</span><span class="ContextInformationVolumeNumber">314</span></span><span class="ChapterDOI"><a href="A468350_1_En_6_Chapter.html">https://doi.org/10.1007/978-3-319-91602-6_6</a></span></div></div><!--Begin Abstract--><div class="MainTitleSection"><h1 class="ChapterTitle" xml:lang="en">Myths and Facts About Static Application Security Testing Tools: An Action Research at Telenor Digital</h1></div><div class="AuthorGroup"><div class="AuthorNames"><span class="Author"><span class="AuthorName">Tosin Daniel Oyetoyan</span><sup>1 <a href="#ContactOfAuthor1"><span class="ContactIcon"> </span></a></sup>, </span><span class="Author"><span class="AuthorName">Bisera Milosheska</span><sup>2 <a href="#ContactOfAuthor2"><span class="ContactIcon"> </span></a></sup>, </span><span class="Author"><span class="AuthorName">Mari Grini</span><sup>2 <a href="#ContactOfAuthor3"><span class="ContactIcon"> </span></a></sup> and </span><span class="Author"><span class="AuthorName">Daniela Soares Cruzes</span><sup>1 <a href="#ContactOfAuthor4"><span class="ContactIcon"> </span></a></sup></span></div><div class="Affiliations"><div class="Affiliation" id="Aff9"><span class="AffiliationNumber">(1)</span><div class="AffiliationText">Department of Software Engineering, Safety and Security, SINTEF Digital, Trondheim, Norway</div></div><div class="Affiliation" id="Aff10"><span class="AffiliationNumber">(2)</span><div class="AffiliationText">Telenor Digital, Oslo, Norway</div></div><div class="ClearBoth"> </div></div><div class="Contacts"><div class="Contact" id="ContactOfAuthor1"><div class="ContactIcon"> </div><div class="ContactAuthorLine"><span class="AuthorName">Tosin Daniel Oyetoyan</span> (Corresponding author)</div><div class="ContactAdditionalLine"><span class="ContactType">Email: </span><a href="mailto:tosin.oyetoyan@sintef.no" class="Email">tosin.oyetoyan@sintef.no</a></div></div><div class="Contact" id="ContactOfAuthor2"><div class="ContactIcon"> </div><div class="ContactAuthorLine"><span class="AuthorName">Bisera Milosheska</span></div><div class="ContactAdditionalLine"><span class="ContactType">Email: </span><a href="mailto:bisera.milosheska@telenordigital.com" class="Email">bisera.milosheska@telenordigital.com</a></div></div><div class="Contact" id="ContactOfAuthor3"><div class="ContactIcon"> </div><div class="ContactAuthorLine"><span class="AuthorName">Mari Grini</span></div><div class="ContactAdditionalLine"><span class="ContactType">Email: </span><a href="mailto:mari@telenordigital.com" class="Email">mari@telenordigital.com</a></div></div><div class="Contact" id="ContactOfAuthor4"><div class="ContactIcon"> </div><div class="ContactAuthorLine"><span class="AuthorName">Daniela Soares Cruzes</span></div><div class="ContactAdditionalLine"><span class="ContactType">Email: </span><a href="mailto:danielac@sintef.no" class="Email">danielac@sintef.no</a></div></div></div></div><div class="Abstract" id="Abs1" xml:lang="en"><div class="Heading">Abstract</div><div id="Par1" class="Para">It is claimed that integrating agile and security in practice is challenging. There is the notion that security is a heavy process, requires expertise, and consumes developers’ time. These contrast with the agile vision. Regardless of these challenges, it is important for organizations to address security within their agile processes since critical assets must be protected against attacks. One way is to integrate tools that could help to identify security weaknesses during implementation and suggest methods to refactor them. We used quantitative and qualitative approaches to investigate the efficiency of the tools and what they mean to the actual users (i.e. developers) at Telenor Digital. Our findings, although not surprising, show that several barriers exist both in terms of tool’s performance and developers’ perceptions. We suggest practical ways for improvement.</div></div><div class="KeywordGroup" xml:lang="en"><div class="Heading">Keywords</div><span class="Keyword">Security defects</span><span class="Keyword">Agile</span><span class="Keyword">Static analysis</span><span class="Keyword">Static application security testing</span><span class="Keyword">Software security</span></div><!--End Abstract--><div class="Fulltext"><div id="Sec1" class="Section1 RenderAsSection1"><h2 class="Heading"><span class="HeadingNumber">1 </span>Introduction</h2><div id="Par2" class="Para">The need and urgency for quality software is higher than any other time in our history because of the rate of interconnection and dependence on software. Society, systems, and businesses are driven by software systems that are integrated into a complex system of systems (e.g. automation systems, business systems, Internet of Things, mobile devices). This is changing the threat landscape continuously. Unfortunately, the rise in consumer software technologies and methodologies for delivering them are not matched with an increase in security investment. This is evidenced in large-scale vulnerability reports and regular breaches [<cite><a href="#CR1">1</a></cite>].</div><div id="Par3" class="Para">Information gathering, exploits and hacking tools [e.g. Kali Linux] are now easily accessible and the ability for an attacker to cause serious damage is more real than ever. On the other side, developers do not code with the mindset of an attacker because they care more about delivering functionalities. Common coding mistakes and inadvertent programming errors are weaknesses that often evolve into exploitable vulnerabilities<a href="#Fn1" id="Fn1_source"><sup>1</sup></a>. It is claimed that, about 70-percent of reported attacks are performed at the application layer rather than the network layer [<cite><a href="#CR12">12</a></cite>].</div><div id="Par5" class="Para">Integrating static analysis tool could be envisaged to help developers code defensively [<cite><a href="#CR26">26</a></cite>]. Tools are important in agile development that values continuous delivery [<cite><a href="#CR21">21</a></cite>]. Static analysis tools (SATs) play important role to ensure product meets the quality requirements. SATs exercise application source code and check them for violations [<cite><a href="#CR8">8</a></cite>]. With respect to security, the decision to implement static analysis tools has to be guided. Using a static analysis tool does not imply an automatic improvement in the security of the code. For instance, teams may use such tools for checking styles, method quality, and maintenance related issues (e.g. duplicated code). These do not translate directly to security, as elegant code can still be vulnerable to attacks [<cite><a href="#CR20">20</a></cite>].</div><div id="Par6" class="Para">The security group at Telenor Digital is focused on integrating security activities in their agile teams. Telenor Digital is a community within Telenor Group, a Norwegian based international telecom operator, working to position Telenor as a digital service provider. As a result, the community researches into new possibilities and develops the next-generation digital solutions for Telenor customers transnationally. Telenor Digital is distributed in Oslo, Trondheim, and Bangkok. Each team has autonomy in its processes and leverages agile development methodologies.</div><div id="Par7" class="Para">One of the steps the security group has taken is to collaborate with the SoS-Agile project<a href="#Fn2" id="Fn2_source"><sup>2</sup></a>, which investigates how to meaningfully integrate software security into agile software development activities. The method of choice for the project is Action Research [<cite><a href="#CR16">16</a></cite>]. The combination of scientific and practical objectives align with the basic tenet of action research, which is to merge theory and practice in a way that real-world problems are solved by theoretically informed actions in collaboration between researchers and practitioners [<cite><a href="#CR16">16</a></cite>]. Therefore, the approach taken has considered the usefulness of the results both for the companies and for research.</div><div id="Par9" class="Para">Since traditional security engineering process is often associated with additional development efforts and as a result often invokes resentment among agile development teams [<cite><a href="#CR5">5</a></cite>]. It is thus important for the security group to approach development teams in a way that guarantees successful integration. This paper investigates the efficiency and developers’ perceptions of static application security testing (SAST) tool within the agile teams at Telenor Digital. Our findings have implications for both practice and research. They show the challenges faced by developers, enumerate practical improvement approaches, and contribute to the body of knowledge about the performance of static analysis tools.</div><div id="Par10" class="Para">The rest of this paper is structured as follows: In Sect. <span class="InternalRef"><a href="#Sec2">2</a></span>, we present the background to the study and our research questions. In Sect. <span class="InternalRef"><a href="#Sec3">3</a></span>, we describe our case study and present the results. Section <span class="InternalRef"><a href="#Sec11">4</a></span> discusses the implications of the study for both practice and research. We present the limitations to the study in Sect. <span class="InternalRef"><a href="#Sec12">5</a></span> and conclude in Sect. <span class="InternalRef"><a href="#Sec13">6</a></span>.</div></div><div id="Sec2" class="Section1 RenderAsSection1"><h2 class="Heading"><span class="HeadingNumber">2 </span>Background</h2><div id="Par11" class="Para">Different studies have investigated why developers do not use static analysis tool to find bugs e.g. [<cite><a href="#CR18">18</a></cite>] or how developers interact with such tools when diagnosing potential security vulnerabilities e.g. [<cite><a href="#CR23">23</a></cite>]. Findings show that false positives and the way warnings are presented are barriers to use. Similarly, deep interaction by developers with tool’s result and several questions they asked highlight another challenge of cognitively demanding tasks that could threaten the use of such tool [<cite><a href="#CR23">23</a></cite>]. Baca et al. [<cite><a href="#CR4">4</a></cite>] evaluated the use of a commercial static analysis tool to improve security in an industrial settings. They found that, although the tool reported some relevant warnings, it was hard for developers to classify them. In addition, developers corrected false positive warnings, which created vulnerabilities in previously safe code. Hofer [<cite><a href="#CR17">17</a></cite>] has used some other metrics to guide tools’ selection such as installation, configuration, support, reports, errors found, and whether the tools can handle a whole project rather than parsing single files.</div><div id="Par12" class="Para">Other researchers have also performed independent quantitative evaluation of static analysis tools with regards to their performances to detect security weaknesses. The Center for Assured Software (CAS) [<cite><a href="#CR19">19</a></cite>] developed a benchmark testsuite with “good code” and “flawed code” across different languages to evaluate the performance of static analysis tools. They assessed 5 commercial tools and reported the highest recall of 0.67 and highest precision of 0.45. Goseva-Popstojanova and Perhinschi [<cite><a href="#CR15">15</a></cite>] investigated the capabilities of 3 commercial tools. Their findings showed that the capability of the tools to detect vulnerabilities was close to or worse than average. Díaz and Bermejo [<cite><a href="#CR10">10</a></cite>] compared the performance of nine tools mostly commercial tools using the SAMATE security benchmark test suites. They found an average recall of 0.527 and average precision of 0.7. They found also that the tools detected different kinds of weaknesses. Charest [<cite><a href="#CR7">7</a></cite>] compared 4 tools against 4 out of the 112 CWEs in the SAMATE Juliet test case. The best average performance in terms of recall was 0.46 for CWE89 with an average precision of 0.21.</div><div id="Par13" class="Para">The methodology employed by the security group and the SoS-Agile research team combined both the qualitative and quantitative approaches. Although, we could learn from the reported studies in the literature, we could not directly apply these results to the organization’s case because of context issue. First, the set of tools that are to be evaluated against the benchmark of our choice are mostly not within the set reported in the literature and in many cases the names of the tools are not disclosed. Second, tools’ evolution over time is also a context factor that makes it reasonable to re-evaluate them even if they have been previously evaluated. Third, the developers in the organization could express specific challenges that might not have been mentioned in the literature but would be important if the security team wants to succeed with introducing a static analysis tool.</div><div id="Par14" class="Para">Therefore, there are 2 research questions that are of interest to the security group at Telenor Digital and the SoS-Agile research team with regards to integrating SAST tools in the organization:</div><div id="Par15" class="Para ParaOneEmphasisChild"><span class="EmphasisTypeBold">RQ1. What are the capabilities of the SAST tools in order to make informed decision for the development team?</span> Implementing SAST tools in a meaningful and useful ways requires evaluating various tools independently in order to make informed decision. We disregard statements from vendors as they can overrate the capability of their tools. We do not distinguish between open source and commercial tools because implementing inefficient tools irrespective of license type has implications with respect to cost, time, and long-term perception/future adoption.</div><div id="Par16" class="Para">Furthermore, different classes of weaknesses are of interest. For instance, how does a SAST tool perform with regards to authentication and authorization weaknesses or with regards to control flow management weaknesses. Such understanding is crucial to know the strengths and weaknesses so that even if a tool is adopted, our knowledge of its strengths would prevents overestimation and a false sense of security and our knowledge of its weaknesses would guide further testing activities later in the development lifecycle.</div><div id="Par17" class="Para ParaOneEmphasisChild"><span class="EmphasisTypeBold">RQ2. How do developers perceive static analysis tools with respect to successful implementation and long-term adoption by teams?</span> Understanding the desired features in SAST tools that could increase the chance of adoption would be important. Likewise, understanding the challenges and developers’ fears regarding new security tools that could lessen the chance of adoption would also be useful. By using this complimentary information, managements have better possibility to improve the chance of adoption by the team.</div></div><div id="Sec3" class="Section1 RenderAsSection1"><h2 class="Heading"><span class="HeadingNumber">3 </span>Case Study</h2><div id="Par18" class="Para">We have used quantitative and qualitative approaches to investigate our research questions. For RQ1, we performed an independent evaluation using a benchmarking approach [<cite><a href="#CR2">2</a></cite>, <cite><a href="#CR19">19</a></cite>] of open source SAST tools and a commercial SAST tool being considered for adoption at the organization. For RQ2, we interviewed 6 developers in one of the teams regarding their perceptions about SAST tool.</div><div id="Sec4" class="Section2 RenderAsSection2"><h3 class="Heading"><span class="HeadingNumber">3.1 </span>Evaluating Static Analysis Tools</h3><div id="Par19" class="Para">Our approach to evaluate SAST tools includes the selection of benchmark testsuites, selection of static analysis tools, running the analysis tools on the testsuites, and presenting the results using performance metrics. Evaluating tools using natural code is very challenging [<cite><a href="#CR14">14</a></cite>]. One challenge is reviewing each result of the tool to determine whether it is correct or not. This is a time consuming activity with no guarantee of correctness. Another is the difficulty to compare results from different tools since they report differently. We thus decided to use an artificial benchmark test suite.</div><div id="Par20" class="Para"><span class="EmphasisTypeBold">Benchmark for Evaluating SAST Tools:</span> Different benchmark test suites exist for testing security tools. Common examples are the OWASP Benchmark [<cite><a href="#CR2">2</a></cite>] and the NIST test suites [<cite><a href="#CR19">19</a></cite>]. We decided for NIST dataset because it is not only limited to top 10 vulnerabilities unlike OWASP benchmark test dataset. In addition, NIST dataset is designed for all range of weaknesses and not only limited to web-based weaknesses like OWASP.</div><div id="Par21" class="Para"><span class="EmphasisTypeBold">NIST Test Suite:</span> The National Institute of Standards and Technology (NIST) Software Assurance Reference Dataset (SARD) Project [<cite><a href="#CR19">19</a></cite>] provides a collection of test suites intended to evaluate the performance of different SAST tools. The test suite uses the common weaknesses and enumeration (CWE) dictionary by MITRE (see footnote 1) and contains artificial bad and good files/methods. The bad file/method contains the actual weakness to be tested by the tool. The good file/method contains no exploitable weakness. Figure <span class="InternalRef"><a href="#Fig1">1</a></span> shows an example of a test case that is vulnerable to cross-site scripting (XSS) attack since the user-supplied value stored in the variable “data” is not properly sanitized before being displayed. However, Fig. <span class="InternalRef"><a href="#Fig2">2</a></span> shows a fix by using a hardcoded value for “data” (trusted input). Although, the sink still contains the weakness that could lead to XSS attack, no user-supplied value is passed to the variable “data”. Therefore, this weakness cannot be exploited. This simple design is valuable to differentiate between tools that only perform string pattern matching against those that use more sophisticated approaches (e.g. control/data-flow analysis). We have used the Juliet Test Suite v1.2 with a total of 26,120 test cases covering 112 different weaknesses (CWEs). In order to compare the tools at a higher granularity level, the CWEs are aggregated into 13 categories as shown in Table <span class="InternalRef"><a href="#Tab1">1</a></span>.<div class="Figure" id="Fig1"><div class="MediaObject" id="MO1"><img src="A468350_1_En_6_Fig1_HTML.gif" alt="A468350_1_En_6_Fig1_HTML.gif"/></div><div class="Caption" xml:lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 1.</span><div class="SimplePara">Bad source and bad sink method for XSS - CWE80</div></div></div></div>
<div class="Figure" id="Fig2"><div class="MediaObject" id="MO2"><img src="A468350_1_En_6_Fig2_HTML.gif" alt="A468350_1_En_6_Fig2_HTML.gif"/></div><div class="Caption" xml:lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 2.</span><div class="SimplePara">Good source and bad sink method for XSS - CWE80</div></div></div></div>
</div><div id="Par22" class="Para ParaOneEmphasisChild"><span class="EmphasisTypeBold">Selected Static Analysis Tools:</span> We have evaluated 5 open source tools (FindBugs, FindSecBugs, SonarQube, JLint, and Lapse+) and a mainstream commercial tool. Commercial tools use proprietary license and are thus challenging for research purposes. The open source tools are selected based on language support, ease of installation and that they can be used to find security flaws. Additionally, FindBugs, FindSecBugs, and SonarQube are widely adopted. The commercial static analysis tool is being considered for adoption at Telenor Digital.</div><div id="Par23" class="Para"><span class="EmphasisTypeBold">Automated Analysis and Comparison:</span> Tools report results in different formats and thus makes the comparison of tools a somewhat cumbersome process. We need to create a uniform format to compare the results from the tools. We adopted the approach by Wagner and Sametinger [<cite><a href="#CR24">24</a></cite>] and transformed each report into a CSV file, where each line contains details about each detected flaw, such as: name of the scanner (tool), abbreviation of the flaw reported by the scanner, name of the file and line number where the flaw was located, as well as the message reported by the scanner. To map the reported flaws from each scanner to their possible CWE codes, we used the CWE XML-mapping file as shown in Fig. <span class="InternalRef"><a href="#Fig3">3</a></span> for each scanner (tool). This file contains the tool’s code for a reported flaw and their possible CWE equivalent. Where vendors do not provide this information, we look for the best possible matching from the CWE database. The flaws reported in the CSV reports for each tool are then mapped to CWE numbers using the scanner’s CWE XML-mapping files.<div id="Tab1" class="Table"><div class="Caption" xml:lang="en"><div class="CaptionContent"><span class="CaptionNumber">Table 1.</span><div class="SimplePara">Weakness categories [<cite><a href="#CR13">13</a></cite>]</div></div></div><table border="1"><colgroup><col align="left"/><col align="left"/><col align="left"/></colgroup><thead><tr class="header"><th align="left"><div class="SimplePara">Weakness class</div></th><th align="left"><div class="SimplePara">Description</div></th><th align="left"><div class="SimplePara">Examples</div></th></tr></thead><tbody><tr class="noclass"><td align="left"><div class="SimplePara">Authentication and Access Control</div></td><td align="left"><div class="SimplePara">Testing for unauthorized access to a system</div></td><td align="left"><div class="SimplePara">CWE-620: Unverified Password Change</div></td></tr><tr class="noclass"><td align="left"><div class="SimplePara">Code Quality</div></td><td align="left"><div class="SimplePara">Issues not typically security related but could lead to performance and maintenance issues</div></td><td align="left"><div class="SimplePara">CWE-478: Omitted Default Case in a Switch</div></td></tr><tr class="noclass"><td align="left"><div class="SimplePara">Control Flow Management</div></td><td align="left"><div class="SimplePara">Timing and synchronization issues</div></td><td align="left"><div class="SimplePara">CWE-362: Race Condition</div></td></tr><tr class="noclass"><td align="left"><div class="SimplePara">Encryption and Randomness</div></td><td align="left"><div class="SimplePara">Weak or wrong encryption algorithms</div></td><td align="left"><div class="SimplePara">CWE-328: Reversible One-Way Hash</div></td></tr><tr class="noclass"><td align="left"><div class="SimplePara">Error Handling</div></td><td align="left"><div class="SimplePara">Failure to handle errors properly that could lead to unexpected consequences</div></td><td align="left"><div class="SimplePara">CWE-252: Unchecked Return Value</div></td></tr><tr class="noclass"><td align="left"><div class="SimplePara">File Handling</div></td><td align="left"><div class="SimplePara">Checks for proper file handling during read and write operations to a file on the hard-disk</div></td><td align="left"><div class="SimplePara">CWE-23: Relative Path Traversal</div></td></tr><tr class="noclass"><td align="left"><div class="SimplePara">Information Leaks</div></td><td align="left"><div class="SimplePara">Unintended leakage of sensitive information</div></td><td align="left"><div class="SimplePara">CWE-534: Information Leak Through Debug Log Files</div></td></tr><tr class="noclass"><td align="left"><div class="SimplePara">Initialization and Shutdown</div></td><td align="left"><div class="SimplePara">Checks for proper initializing and shutting down of resources</div></td><td align="left"><div class="SimplePara">CWE-415: Double Free</div></td></tr><tr class="noclass"><td align="left"><div class="SimplePara">Injection</div></td><td align="left"><div class="SimplePara">Input validation weaknesses</div></td><td align="left"><div class="SimplePara">CWE-89: SQL Injection</div></td></tr><tr class="noclass"><td align="left"><div class="SimplePara">Malicious Logic</div></td><td align="left"><div class="SimplePara">Implementation of a program that performs an unauthorized or harmful action (e.g. worms, backdoors)</div></td><td align="left"><div class="SimplePara">CWE-506: Embedded Malicious Code</div></td></tr><tr class="noclass"><td align="left"><div class="SimplePara">Miscellaneous</div></td><td align="left"><div class="SimplePara">Other weaknesses types not in the defined categories</div></td><td align="left"><div class="SimplePara">CWE-482: Comparing instead of Assigning</div></td></tr><tr class="noclass"><td align="left"><div class="SimplePara">Number Handling</div></td><td align="left"><div class="SimplePara">Incorrect calculations, number storage, and conversion weaknesses</div></td><td align="left"><div class="SimplePara">CWE-369: Divide by Zero</div></td></tr><tr class="noclass"><td align="left"><div class="SimplePara">Pointer and Reference Handling</div></td><td align="left"><div class="SimplePara">Proper pointer and reference handling</div></td><td align="left"><div class="SimplePara">CWE-476: Null Pointer Dereference</div></td></tr></tbody></table></div>
</div><div id="Par24" class="Para">
                  <div class="Figure" id="Fig3"><div class="MediaObject" id="MO3"><img src="A468350_1_En_6_Fig3_HTML.gif" alt="A468350_1_En_6_Fig3_HTML.gif"/></div><div class="Caption" xml:lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 3.</span><div class="SimplePara">XML mapping of tools to CWE</div></div></div></div>
                </div><div id="Par25" class="Para">We automate some parts of the process and manually process the other parts due to how the tools can be configured and accessed (e.g. through a command line, user interface or integrated development environment) and the different operating systems they support. For example, only FindBugs, FindSecBugs, and SonarQube could be executed via command line on OS X platform. JLint is only compatible with Windows OS and for Lapse+, we have to generate the result through the IDE.</div><div id="Par26" class="Para">We have used the tool in [<cite><a href="#CR24">24</a></cite>] for tools accessible via command line. The tool did not perform recursive scanning of files in subfolders and thus missed several of the test suite files. We fixed this serious bug and provided an extension of the tool<a href="#Fn3" id="Fn3_source"><sup>3</sup></a>. For Lapse+ and Commercial tool, we processed the reports separately and converted them to the uniform CSV format because of platform differences. Lastly, we developed additional Java tool to compute the performance metrics to fit the metrics originally defined by CAS [<cite><a href="#CR13">13</a></cite>].</div></div><div id="Sec5" class="Section2 RenderAsSection2"><h3 class="Heading"><span class="HeadingNumber">3.2 </span>Performance Metrics</h3><div id="Par28" class="Para">We use the following performance metrics [<cite><a href="#CR13">13</a></cite>].</div><div id="Par29" class="Para ParaOneEmphasisChild"><span class="EmphasisTypeBold">True Positive (TP):</span> The number of cases where the tool correctly reports the flaw that is the target of the test case.</div><div id="Par30" class="Para ParaOneEmphasisChild"><span class="EmphasisTypeBold">False Positive (FP):</span> The number of cases where tool reports a flaw with a type that is the target of the test case, but the flaw is reported in non-flawed code.</div><div id="Par31" class="Para ParaOneEmphasisChild"><span class="EmphasisTypeBold">False Negative (FN):</span> This is not a tool result. A false negative result is added for each test case for which there is no true positive.</div><div id="Par32" class="Para ParaOneEmphasisChild"><span class="EmphasisTypeBold">Discrimination:</span> The number of cases where tool correctly reports the flaw and does not report the non-flaw (i.e. TP = 1 and FP = 0). The discrimination rate is usually equal or lower than the TP rate (Recall).</div><div id="Par33" class="Para ParaOneEmphasisChild"><span class="EmphasisTypeBold">Blank (Incidental flaws):</span> This represents tool’s result where none of the types above apply. More specifically, either the tool’s result is not in a test case file or the tool’s result is not associated with the test case in which it is reported.</div><div id="Par34" class="Para">
                  <div class="UnorderedList"><ul class="UnorderedListMarkDash"><li><div id="Par35" class="Para"><span class="EmphasisTypeItalic">Recall</span> = <span class="InlineEquation" id="IEq1"><img src="A468350_1_En_6_Chapter_IEq1.gif" alt="$$\frac{TP}{TP+FN}$$"/></span></div></li><li><div id="Par36" class="Para"><span class="EmphasisTypeItalic">Precision</span> = <span class="InlineEquation" id="IEq2"><img src="A468350_1_En_6_Chapter_IEq2.gif" alt="$$\frac{TP}{TP+FP}$$"/></span></div></li><li><div id="Par37" class="Para"><span class="EmphasisTypeItalic">DiscriminationRate</span> = <span class="InlineEquation" id="IEq3"><img src="A468350_1_En_6_Chapter_IEq3.gif" alt="$$\frac{\#Discriminations}{TP+FN}$$"/></span></div></li></ul></div>
                </div><div id="Par38" class="Para">It is possible to have both TP and FP in the same file as shown in Fig. <span class="InternalRef"><a href="#Fig2">2</a></span>. In this case, the tool is not sophisticated enough to discriminate for instance when data source is hardcoded and therefore does not need to be sanitized. When we compute discrimination, we are only concerned with cases when the tool reports TP. We set the discrimination to 1 if it does not report FP on the same file.</div><div id="Par39" class="Para">We adopt the “strict” metrics defined by CAS [<cite><a href="#CR13">13</a></cite>] as they truly reflect real-world situation. For instance, Wagner and Sametinger [<cite><a href="#CR24">24</a></cite>] modified this metrics by accepting tools’ detection in the “non-flaw” part of the code as valid as long as they are reported in the target CWE file. While this modification may make a tool’s performance look better, in the true sense, it does not reflect how developers interact with tool’s report. Precision of reported issue in a file is important otherwise it might lead to confusion and cognitive stress when developers try to make sense of it.</div></div><div id="Sec6" class="Section2 RenderAsSection2"><h3 class="Heading"><span class="HeadingNumber">3.3 </span>Results of Tools’ Evaluation</h3><div id="Par40" class="Para">We report the evaluation results of the 6 tools on Juliet Test Suite v1.2. As shown in Table <span class="InternalRef"><a href="#Tab2">2</a></span> and Fig. <span class="InternalRef"><a href="#Fig4">4</a></span>, FindSecBugs records the highest recall of 18.4% with approximately 90% precision. It also has the highest discrimination rate, which is slightly lower than its recall. Lapse+ follows with a detection rate of 9.76% but with poor discrimination rate of 0.41%. However, when we break down the result into different weakness categories, this number was found only in “File Handling” and “Injection” weaknesses. The results from the Commercial tool is not as competitive as it ranked third. However, results in the categories revealed certain areas where the tool could be ahead of others.</div><div id="Par41" class="Para">The tools reported several other warnings, which are recorded under “incidental flaws”. These warnings are not the target of the test but they indicate the “noise” levels of the tools. Many of the warnings could be categorized as “trivial” when compared with security issues. An example is warning about code styling.</div><div id="Par42" class="Para">We made the following observations under each weakness category (see Table <span class="InternalRef"><a href="#Tab3">3</a></span>):<div id="Tab2" class="Table"><div class="Caption" xml:lang="en"><div class="CaptionContent"><span class="CaptionNumber">Table 2.</span><div class="SimplePara">Number of identified weaknesses by tools from a total of 26120 flaws</div></div></div><table border="1"><colgroup><col align="left"/><col align="left"/><col align="left"/><col align="left"/><col align="left"/></colgroup><thead><tr class="header"><th align="left"><div class="SimplePara">Tool</div></th><th align="left"><div class="SimplePara">TP</div></th><th align="left"><div class="SimplePara">FP</div></th><th align="left"><div class="SimplePara">#Discrimination</div></th><th align="left"><div class="SimplePara">Incidental flaws</div></th></tr></thead><tbody><tr class="noclass"><td align="left"><div class="SimplePara">SonarQube</div></td><td align="left"><div class="SimplePara">1292</div></td><td align="left"><div class="SimplePara">1275</div></td><td align="left"><div class="SimplePara">200</div></td><td align="left"><div class="SimplePara">237845</div></td></tr><tr class="noclass"><td align="left"><div class="SimplePara">Commercial</div></td><td align="left"><div class="SimplePara">2038</div></td><td align="left"><div class="SimplePara">3834</div></td><td align="left"><div class="SimplePara">1085</div></td><td align="left"><div class="SimplePara">360212</div></td></tr><tr class="noclass"><td align="left"><div class="SimplePara">FindSecBugs</div></td><td align="left"><div class="SimplePara">4811</div></td><td align="left"><div class="SimplePara">604</div></td><td align="left"><div class="SimplePara">4338</div></td><td align="left"><div class="SimplePara">41637</div></td></tr><tr class="noclass"><td align="left"><div class="SimplePara">Lapse+</div></td><td align="left"><div class="SimplePara">2550</div></td><td align="left"><div class="SimplePara">2736</div></td><td align="left"><div class="SimplePara">108</div></td><td align="left"><div class="SimplePara">18950</div></td></tr><tr class="noclass"><td align="left"><div class="SimplePara">JLint</div></td><td align="left"><div class="SimplePara">125</div></td><td align="left"><div class="SimplePara">26</div></td><td align="left"><div class="SimplePara">104</div></td><td align="left"><div class="SimplePara">586</div></td></tr><tr class="noclass"><td align="left"><div class="SimplePara">FindBugs</div></td><td align="left"><div class="SimplePara">426</div></td><td align="left"><div class="SimplePara">98</div></td><td align="left"><div class="SimplePara">341</div></td><td align="left"><div class="SimplePara">22245</div></td></tr></tbody></table></div>
</div><div id="Par43" class="Para">
                  <div class="Figure" id="Fig4"><div class="MediaObject" id="MO4"><img src="A468350_1_En_6_Fig4_HTML.gif" alt="A468350_1_En_6_Fig4_HTML.gif"/></div><div class="Caption" xml:lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 4.</span><div class="SimplePara">Overall performance results from the tools</div></div></div></div>
                  <div id="Tab3" class="Table"><div class="Caption" xml:lang="en"><div class="CaptionContent"><span class="CaptionNumber">Table 3.</span><div class="SimplePara">Performance of tools against weakness categories</div></div></div><div class="MediaObject" id="MO5"><img src="A468350_1_En_6_Figa_HTML.gif" alt="A468350_1_En_6_Figa_HTML.gif"/></div></div>
                </div><div id="Par44" class="Para ParaOneEmphasisChild"><span class="EmphasisTypeBold">Authentication and Authorization:</span> FindSecBugs has the best detection rate of 57.20% and followed by “Commercial” tool with 29.39%. The discrimination rate is as good as the detection rate for all the tools. Both JLint and Lapse+ did not detect any weakness in this category.</div><div id="Par45" class="Para ParaOneEmphasisChild"><span class="EmphasisTypeBold">Number Handling:</span> None of the tools could detect the weaknesses under this category. The tools report different issues in the “Number Handling” CWE files, which are not the actual weaknesses. This was alarming and indicates that manual code review in addition to automatic analysis by tool should be performed for number handling weaknesses (e.g. division by zero).</div><div id="Par46" class="Para ParaOneEmphasisChild"><span class="EmphasisTypeBold">File Handling:</span> Lapse+ produced the best detection rate of 58.35%. However, only 2.54% (discrimination rate) is correctly identified without flagging warning simultaneously in the “bad code” construct. Apart from Lapse+, only FindBugs and FindSecBugs could detect weaknesses in this category with a detection rate of 3.59%.</div><div id="Par47" class="Para ParaOneEmphasisChild"><span class="EmphasisTypeBold">Miscellaneous:</span> JLint recorded the best performance under miscellaneous (e.g. CWE-482: Comparing instead of Assigning) category with a recall and discrimination rate of 19.32%. Commercial tool and FindBugs have detection rates of 9.09% and 8.09% respectively. SonarQube, Lapse+ and FindSecBugs did not detect any weakness in this category.</div><div id="Par48" class="Para ParaOneEmphasisChild"><span class="EmphasisTypeBold">Code Quality:</span> The tools’ performance is surprisingly low in this category. The highest recall of 6.8% were recorded by FindSecBugs and FindBugs.</div><div id="Par49" class="Para ParaOneEmphasisChild"><span class="EmphasisTypeBold">Control Flow Management:</span> FindBugs, FindSecBugs, and Commercial tool detected some issues in this category. However, FindSecBugs and FindBugs detection rate is 11.36 times better than the commercial tool.</div><div id="Par50" class="Para ParaOneEmphasisChild"><span class="EmphasisTypeBold">Encryption and Randomness:</span> FindSecBugs has the highest detection rate of 22.83% but with very low discrimination rate of 2.73%. SonarQube detected 16.40% issues, while Commercial tool detected 1.93% issues. The remaining 3 tools did not find any issue in this category.</div><div id="Par51" class="Para ParaOneEmphasisChild"><span class="EmphasisTypeBold">Error Handling:</span> Only SonarQube and FindBugs detected weaknesses in this category. SonarQube has a detection rate of 25.35% and FindBugs has 11.97% detection rate.</div><div id="Par52" class="Para ParaOneEmphasisChild"><span class="EmphasisTypeBold">Information Leaks:</span> Only the Commercial tool detected weaknesses in this category with a detection rate of 9.57%</div><div id="Par53" class="Para ParaOneEmphasisChild"><span class="EmphasisTypeBold">Initialization and Shutdown:</span> The performances of the tools are very poor in this category. Four tools (SonarQube, Commercial, FindBugs, and FindSecBugs) detected some weaknesses with the highest detection rate of 0.57%.</div><div id="Par54" class="Para ParaOneEmphasisChild"><span class="EmphasisTypeBold">Injection:</span> JLint did not find any issue in this category. FindSecBugs has the highest detection rate of 38%, followed by Lapse+ at 18.85% but with poor discrimination rate of 0.79% and Commercial tool with 16.07%.</div><div id="Par55" class="Para ParaOneEmphasisChild"><span class="EmphasisTypeBold">Malicious Logic:</span> Only Commercial tool and FindSecBugs detected weaknesses under this category. The highest detection rate is 4.17% by FindSecBugs while commercial tool only detected 1.23% of the weaknesses.</div><div id="Par56" class="Para ParaOneEmphasisChild"><span class="EmphasisTypeBold">Pointer and Reference Handling:</span> Lapse+ did not detect any weakness in this category. FindBugs, SonarQube, FindSecBugs, and JLint have relatively similar detection rate of about 20%. However, only FindSecBugs showed the best discrimination power of 19.5%. Commercial tool detection rate is 50% lower than the rest of the tools.</div></div><div id="Sec7" class="Section2 RenderAsSection2"><h3 class="Heading"><span class="HeadingNumber">3.4 </span>Interview</h3><div id="Par57" class="Para">We have interviewed 6 out of the 8 developers in the selected team. The interview was divided into 5 sections. The first part covered the professional background such as job title, years of programming experience, and whether they had security related experiences. The second part covered personal opinion on their expectations and challenges with implementing SAST tools. It also included questions about their current practices. The third covered their development approaches. For instance software development methodology, release cycles, and refactoring practices. The fourth part concerned questions about development environments and the last part covered team’s quality assurance and security practices.</div></div><div id="Sec8" class="Section2 RenderAsSection2"><h3 class="Heading"><span class="HeadingNumber">3.5 </span>Practices and Demography</h3><div id="Par58" class="Para">The team is composed of developers that specialize in separate functionalities, such as business support, system integration, hosting, setup and infrastructure. They use a combination of Scrum and Kanban and perform sprint review every two weeks.</div><div id="Par59" class="Para">The goals of the review are to: keep track of project objectives, define the scope of the next sprint, define a set of tasks that should be included in the next iteration, and perform time estimation for those tasks. From privacy and information security point of view, the developers mentioned that they store sensitive personal data; such as personal messages and voice conversations and these assets are the most critical part of their software. Any security weakness that leads to an unauthorized disclosure or modification of the customers’ highly sensitive information can be damaging to the customers and their business.</div><div id="Par60" class="Para">Quality assurance is performed in multiple phases starting from the design phase of the software development life-cycle (SDLC), when the team discusses potential issues. The team codes mainly in Java and uses common coding standards for Java and additional standards proposed at Telenor Digital. They perform code review, unit and acceptance testing. Lastly they perform continuous refactoring of their code.</div><div id="Par61" class="Para">Despite all these practices, there is no specific focus on security testing of the products. Potential security threats are discussed during the design phase of the SDLC and bad practices are avoided while coding. The team is, however, aware of some design issues they have to fix, such as securing confidential and sensitive logs and as a result, they desire to have automatic security analysis on a regular basis. The developers are free to choose the development software platform they are most comfortable with. Therefore, they develop on all the three major OS platforms: OS X, Windows and Linux. They use various integrated development environments (IDEs), such as IntelliJ, NetBeans, Emacs, Eclipse, and Sublime. Their software is mostly written in Java, but they also develop parts of it in JavaScript, shell script and Python. Jenkins<a href="#Fn4" id="Fn4_source"><sup>4</sup></a> is used as a build server for continuous integration.</div></div><div id="Sec9" class="Section2 RenderAsSection2"><h3 class="Heading"><span class="HeadingNumber">3.6 </span>Experience with Static Analysis Tools and Security</h3><div id="Par63" class="Para">The team is composed of developers with 4 to 37 years of development experience (see Table <span class="InternalRef"><a href="#Tab4">4</a></span>). The developers mentioned that they have used a static analysis tool called sonar before. However, this tool was used for checking code quality such as styling, standards, and large methods. One developer said: <span class="EmphasisTypeItalic">“...We use something called Sonar, ..., It’s good for finding complexity in software, like referential loops ..., Bad style, non-conformance to coding standard, methods that are large or complex, ...”</span>. The developers stated not to have used the tool to find specific security weaknesses. Although they are somehow familiar with vulnerabilities, nearly all indicated little experience with using static analysis tools specifically for security audits.<div id="Tab4" class="Table"><div class="Caption" xml:lang="en"><div class="CaptionContent"><span class="CaptionNumber">Table 4.</span><div class="SimplePara">Professional background and experiences</div></div></div><table border="1"><colgroup><col align="left"/><col align="left"/><col align="left"/></colgroup><thead><tr class="header"><th align="left"><div class="SimplePara">Title</div></th><th align="left"><div class="SimplePara">Programming experience (years)</div></th><th align="left"><div class="SimplePara">Familiarity with security vulnerabilities (scale: 1–5)</div></th></tr></thead><tbody><tr class="noclass"><td align="left"><div class="SimplePara">Software engineer</div></td><td align="left"><div class="SimplePara">4</div></td><td align="left"><div class="SimplePara">2</div></td></tr><tr class="noclass"><td align="left"><div class="SimplePara">Senior software engineer</div></td><td align="left"><div class="SimplePara">18</div></td><td align="left"><div class="SimplePara">3</div></td></tr><tr class="noclass"><td align="left"><div class="SimplePara">Senior software engineer</div></td><td align="left"><div class="SimplePara">37</div></td><td align="left"><div class="SimplePara">3</div></td></tr><tr class="noclass"><td align="left"><div class="SimplePara">Senior software engineer</div></td><td align="left"><div class="SimplePara">20</div></td><td align="left"><div class="SimplePara">3–4</div></td></tr><tr class="noclass"><td align="left"><div class="SimplePara">Senior software engineer</div></td><td align="left"><div class="SimplePara">20</div></td><td align="left"><div class="SimplePara">3</div></td></tr><tr class="noclass"><td align="left"><div class="SimplePara">Software engineer</div></td><td align="left"><div class="SimplePara">6</div></td><td align="left"><div class="SimplePara">4</div></td></tr></tbody></table></div>
</div></div><div id="Sec10" class="Section2 RenderAsSection2"><h3 class="Heading"><span class="HeadingNumber">3.7 </span>Perceptions of the Developers About SAST Tools</h3><div id="Par64" class="Para"><span class="EmphasisTypeBold">Setting Up the Tool and Getting it to Work.</span> The developers fear the effort to setup a third party SAST tool and get it to work seamlessly in their development and build environments. One of the developers who has experience with previous tool said: <span class="EmphasisTypeItalic">“...Making the things actually work, that usually is the worst thing. The hassle-factor is not to be underestimated...”</span>. Both Emanuelsson and Nilsson [<cite><a href="#CR11">11</a></cite>] and Hofer [<cite><a href="#CR17">17</a></cite>] report on installation as a seemingly important metric when choosing a static analysis tool.</div><div id="Par65" class="Para"><span class="EmphasisTypeBold">Invasiveness and Disruption to Regular Workflow.</span> Alerts may distract and interrupt the developer’s flow and can also be a time consuming activity. The developers are clear about the fact that acting on the issues reported from the tool would depend on whether it does not overburden them. They fear that the tool may disrupt the flow of their work. One of the developers said: <span class="EmphasisTypeItalic">“...It depends a lot on the tool and how easy it is to use and how it flows into your regular workflow,...”</span></div><div id="Par66" class="Para"><span class="EmphasisTypeBold">False Positives or Trivial Issues.</span> The developers were unanimous about their concerns with false positives. They are concerned about the tool reporting high number of trivial or unnecessary issues. For instance, one of the developers said: <span class="EmphasisTypeItalic">“...At least from my experience with the Sonar tool is that it sometimes complains about issues that are not really issues...”</span></div><div id="Par67" class="Para"><span class="EmphasisTypeBold">Cognitive Effort to Understand Tool’s Messages.</span> This is a concern to the developers. They would want to use the tool with minimum amount of cognitive effort. It should not be very difficult to understand the message or vocabulary used by the tool. A developer said: <span class="EmphasisTypeItalic">“...What I fear is if they make it necessary to engage mentally a lot in the tool, as to the messages it uses then I would be reluctant to use it...”</span></div><div id="Par68" class="Para"><span class="EmphasisTypeBold">Broad Range of Programming Languages.</span> The developers point out the challenge of supporting several programming languages. They develop using several languages and foresee that it might be challenging to generate static analysis warnings for each of the languages. A developer said: <span class="EmphasisTypeItalic">“...We have several software languages that we write in. Predominantly Java and Javascript. But also some C++ as well. So to target each of those different languages would be an issue ...”</span></div><div id="Par69" class="Para"><span class="EmphasisTypeBold">Huge Technical Debts.</span> One of the challenges expressed is having a huge technical debt after running an implemented static analysis tool. The team rushed their products into the market the previous year and thus fears the possibility that the tool would flag many issues for refactoring. A developer says: <span class="EmphasisTypeItalic">“...and the problem is that when you set it up at this stage of the project we have a huge design debt, because I guess things were implemented quickly, rushed before summer last year...”</span></div></div></div><div id="Sec11" class="Section1 RenderAsSection1"><h2 class="Heading"><span class="HeadingNumber">4 </span>Discussions and Implications</h2><div id="Par70" class="Para">Based on the results from the interview and independent tools evaluation, we discuss the implications of our findings.</div><div id="Par71" class="Para"><span class="EmphasisTypeBold">One Tool Is Not Enough:</span> We found that using one SAST tool is not enough to cover the whole range of security weaknesses at the implementation phase. This is synonymous with the findings by Austin and Williams [<cite><a href="#CR3">3</a></cite>] that compares different techniques across implementation and verification stages. It becomes obvious that developers have to tradeoff on some of their requirements. For instance, full language support might not be covered by one single tool and a single tool that covers many languages might suffer from low performances in many of them. Future research should focus on how to harmonize results from different tools for maximum performance.</div><div id="Par72" class="Para ParaOneEmphasisChild"><span class="EmphasisTypeBold">Tools’ Capability Is Low:</span> The capability of the tools is generally low with regards to detecting security weaknesses in the Juliet Java code. The commercial tool, although highly rated in the market is not an exception. This is very challenging for developers, as they need to focus on important security warnings and disregard the noise. One helpful way we found is to filter the results by using the CWE tag provided by some of the tools. For example, FindSecBugs, SonarQube and the Commercial tool provide support for this feature.</div><div id="Par73" class="Para"><span class="EmphasisTypeBold">Static Analysis Results Are Non-functional Requirements:</span> Developers have hidden bias when it comes to fixing issues reported by static analysis tools. Statements such as: <span class="EmphasisTypeItalic">“...if you are just looking for functionality and spend a lot of time on making your system secure or safe and doing things that you are not getting paid for or the customers are not willing to pay for...”</span> <span class="EmphasisTypeBold">and</span> <span class="EmphasisTypeItalic">“...And of course in itself is not productive, nobody gives you a hug after fixing SonarQube reports,...”</span> demonstrate the challenges and need for making security as part of the development process and in a seamless manner. It shows a need for a top down approach where product owners (POs) are able to prioritize security and include it in the developers’ workflow. Since static analysis reports are non-functional requirements and not features, they never become user story in many cases in agile settings. However, it is possible to adopt the approach in Rindell et al. [<cite><a href="#CR22">22</a></cite>] by moving relevant tool’s report into the product backlog.</div><div id="Par74" class="Para"><span class="EmphasisTypeBold">Do Not Underestimate Integration Effort</span>: Developers are wary of tools that take lots of effort to integrate. This is understandable, as it has cost implication both at the present and in the future. For instance, it would require increased effort to upgrade such tool if something breaks in it. An approach taken by Telenor Digital is to dedicate a resource person as responsible for tools’ implementations, configurations, and maintenance. This is beneficial as it prevents the <span class="EmphasisTypeItalic">“hassle-factor”</span> and allows the agile team to focus squarely on business delivery.</div><div id="Par75" class="Para ParaOneEmphasisChild"><span class="EmphasisTypeBold">Developers Are Positive to Have a SAST Tool:</span> On the other hand, the developers agree that implementing a SAST tool would improve the security of their product. Some are very enthusiastic to learn new things and to get immediate feedback when mistakes are made and learn more about language and platform internals. These would be possible if the tools are able to point out real issues, if it is possible to mark part of the code that should not be scanned, if it is automated and easy to use, if it is not cognitively demanding to interact with the tool, and if the tools report in a way that developers understand.</div><div id="Par76" class="Para"><span class="EmphasisTypeBold">Collaboration Between Researchers and Practitioners:</span> Practitioners sometimes view research-oriented studies to be costly and time consuming. As a result, practitioners could be skeptical to collaborate. However, collaboration between researchers and practitioners can be important and useful drivers to meaningfully improve security in practice. From the perspective of the security group at Telenor Digital, the study was valuable to provide insights both qualitatively and quantitatively and to also drive future decisions. The statement by the head of the security team confirmed this: <span class="EmphasisTypeItalic">“...But I have in particular now noted that it might not be sufficient with only one tool and that it might be more important than we anticipated before this study to understand strengths and weaknesses of the different available tools for static analysis. I also noticed that several open source tools seem to have strengths worthwhile taking into account....”</span></div><div id="Par77" class="Para"><span class="EmphasisTypeBold">Advice for Future Integrators:</span> One major challenge with integrating security activities in agile is the dichotomy between the security professionals and developers [<cite><a href="#CR6">6</a></cite>]. Security activities are often perceived by developers to be time consuming. While the traditional assurance practice dictates to maintain independence between security professionals and developers in order to be objective and neutral [<cite><a href="#CR25">25</a></cite>]. This is confirmed through the use of third-party consultants by some of the teams at Telenor Digital to perform penetration testing for their applications [<cite><a href="#CR9">9</a></cite>]. The security team at Telenor Digital has similar challenges with bridging this gap. The approach used in this study was helpful to allow the security team understands how the developers perceive security activities and what are the important factors that could motivate to adopt them.</div><div id="Par78" class="Para">It is also important to warn that there is a cost for implementing inefficient tools. If there is no benefit from the tool, developers would not use it and this may also affect future possibility to adopt new tool. It is very important to let developers become aware of the strengths and weaknesses of the tools early so that they can have a realistic expectation. It is obvious that today’s SAST tools still need lots of improvements to become better with catching implementation security bugs. However, it is very helpful when developers are part of the decision making such that they know the capability of the tools. This collective “ownership” culture of agile method [<cite><a href="#CR6">6</a></cite>, <cite><a href="#CR25">25</a></cite>] is the approach undertaken at Telenor Digital to introduce and implement a new static application security testing tool for their agile teams.</div></div><div id="Sec12" class="Section1 RenderAsSection1"><h2 class="Heading"><span class="HeadingNumber">5 </span>Limitations</h2><div id="Par79" class="Para ParaOneEmphasisChild"><span class="EmphasisTypeBold">Size and Subjectivity:</span> Interview subjects are few with different experiences and perceptions about static analysis tools. We can therefore not generalize the results.</div><div id="Par80" class="Para ParaOneEmphasisChild"><span class="EmphasisTypeBold">Type of Benchmark:</span> We have used artificial Java code for our evaluation, it is thus possible that real-code and different languages produce different results.</div><div id="Par81" class="Para ParaOneEmphasisChild"><span class="EmphasisTypeBold">Size of Tools:</span> We have used a few number of tools including a very popular commercial tool, however, other tools may present different results to what we have reported.</div><div id="Par82" class="Para ParaOneEmphasisChild"><span class="EmphasisTypeBold">Size/Type of Organization:</span> The organization where this study is carried out is medium-sized and as a result, stakeholders in smaller organizations or startups may express different perceptions.</div><div id="Par83" class="Para ParaOneEmphasisChild"><span class="EmphasisTypeBold">Literature Review:</span> Our pre-study review was conducted informally and not systematically.</div></div><div id="Sec13" class="Section1 RenderAsSection1"><h2 class="Heading"><span class="HeadingNumber">6 </span>Conclusion</h2><div id="Par84" class="Para">We have investigated developers’ perceptions and efficiency of static analysis tools for finding security bugs. We found several barriers exist for adoption by teams such as tools’ low performance, technical debts when implemented late, non-functional nature of security bugs, and the need for many tools. However, teams are positive to use SAST tool to reduce security bugs. We recommend onboarding development teams to learn about the capability of prospective tools and to create synergy between them and the security team.</div></div><div class="Acknowledgments"><div class="Heading">Acknowledgements</div><div class="SimplePara">The work in this paper was carried out at Telenor Digital with support by the SoS-Agile team. The SoS-Agile project is supported by the Research Council of Norway through the project SoS-Agile: Science of Security in Agile Software Development (247678/O70).</div></div><div class="License LicenseSubType-cc-by"><a href="https://creativecommons.org/licenses/by/4.0"><img src="cc-by.png" alt="Creative Commons"/></a><div class="SimplePara">Open Access This chapter is licensed under the terms of the Creative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/), which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license and indicate if changes were made.
The images or other third party material in this book are included in the book's Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the book's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.</div></div><div class="Bibliography" id="Bib1"><div class="Heading">References</div><div class="BibliographyWrapper"><div class="Citation"><div class="CitationNumber">1.</div><div class="CitationContent" id="CR1">Bugtraq mailing list. <span class="ExternalRef"><a href="http://seclists.org/bugtraq/"><span class="RefSource">http://​seclists.​org/​bugtraq/​</span></a></span>. Accessed 10 May 2017</div></div><div class="Citation"><div class="CitationNumber">2.</div><div class="CitationContent" id="CR2">Owasp. benchmark. <span class="ExternalRef"><a href="https://www.owasp.org/index.php/Benchmark"><span class="RefSource">https://​www.​owasp.​org/​index.​php/​Benchmark</span></a></span>. Accessed 20 Oct 2016</div></div><div class="Citation"><div class="CitationNumber">3.</div><div class="CitationContent" id="CR3">Austin, A., Williams, L.: One technique is not enough: a comparison of vulnerability discovery techniques. In: 2011 International Symposium on Empirical Software Engineering and Measurement (ESEM), pp. 97–106. IEEE (2011)</div></div><div class="Citation"><div class="CitationNumber">4.</div><div class="CitationContent" id="CR4">Baca, D., Carlsson, B., Petersen, K., Lundberg, L.: Improving software security with static automated code analysis in an industry setting. Softw. Pract. Exp. <span class="EmphasisTypeBold">43</span>(3), 259–279 (2013)<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a href="https://doi.org/10.1002/spe.2109"><span><span>Crossref</span></span></a></span></span></div></div><div class="Citation"><div class="CitationNumber">5.</div><div class="CitationContent" id="CR5">ben Othmane, L., Angin, P., Weffers, H., Bhargava, B.: Extending the agile development process to develop acceptably secure software. IEEE Trans. Dependable Secur. Comput. <span class="EmphasisTypeBold">11</span>(6), 497–509 (2014)<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a href="https://doi.org/10.1109/TDSC.2014.2298011"><span><span>Crossref</span></span></a></span></span></div></div><div class="Citation"><div class="CitationNumber">6.</div><div class="CitationContent" id="CR6">Beznosov, K., Kruchten, P.: Towards agile security assurance. In: Proceedings of the 2004 Workshop on New Security Paradigms, pp. 47–54. ACM (2004)</div></div><div class="Citation"><div class="CitationNumber">7.</div><div class="CitationContent" id="CR7">Charest, N.R.T., Wu, Y.: Comparison of static analysis tools for Java using the Juliet test suite. In: 11th International Conference on Cyber Warfare and Security, pp. 431–438 (2016)</div></div><div class="Citation"><div class="CitationNumber">8.</div><div class="CitationContent" id="CR8">Chess, B., McGraw, G.: Static analysis for security. IEEE Secur. Privacy <span class="EmphasisTypeBold">2</span>(6), 76–79 (2004)<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a href="https://doi.org/10.1109/MSP.2004.111"><span><span>Crossref</span></span></a></span></span></div></div><div class="Citation"><div class="CitationNumber">9.</div><div class="CitationContent" id="CR9">Soares Cruzes, D., Felderer, M., Oyetoyan, T.D., Gander, M., Pekaric, I.: How is security testing done in agile teams? A cross-case analysis of four software teams. In: Baumeister, H., Lichter, H., Riebisch, M. (eds.) XP 2017. LNBIP, vol. 283, pp. 201–216. Springer, Cham (2017). <span class="ExternalRef"><a href="https://doi.org/10.1007/978-3-319-57633-6_13"><span class="RefSource">https://​doi.​org/​10.​1007/​978-3-319-57633-6_​13</span></a></span><span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a href="https://doi.org/10.1007/978-3-319-57633-6_13"><span><span>Crossref</span></span></a></span></span></div></div><div class="Citation"><div class="CitationNumber">10.</div><div class="CitationContent" id="CR10">Díaz, G., Bermejo, J.R.: Static analysis of source code security: assessment of tools against samate tests. Inf. Softw. Technol. <span class="EmphasisTypeBold">55</span>(8), 1462–1476 (2013)<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a href="https://doi.org/10.1016/j.infsof.2013.02.005"><span><span>Crossref</span></span></a></span></span></div></div><div class="Citation"><div class="CitationNumber">11.</div><div class="CitationContent" id="CR11">Emanuelsson, P., Nilsson, U.: A comparative study of industrial static analysis tools. Electron. Notes Theor. Comput. Sci. <span class="EmphasisTypeBold">217</span>, 5–21 (2008)<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a href="https://doi.org/10.1016/j.entcs.2008.06.039"><span><span>Crossref</span></span></a></span></span></div></div><div class="Citation"><div class="CitationNumber">12.</div><div class="CitationContent" id="CR12">Fong, E., Okun, V.: Web application scanners: definitions and functions. In: 40th Annual Hawaii International Conference on System Sciences, 2007, HICSS 2007, pp. 280b–280b. IEEE (2007)</div></div><div class="Citation"><div class="CitationNumber">13.</div><div class="CitationContent" id="CR13">Center for Assured Software. CAS static analysis tool study - methodology. <span class="ExternalRef"><a href="https://samate.nist.gov/docs/CAS%202012%20Static%20Analysis%20Tool%20Study%20Methodology.pdf"><span class="RefSource">https://​samate.​nist.​gov/​docs/​CAS%20​2012%20​Static%20​Analysis%20​Tool%20​Study%20​Methodology.​pdf</span></a></span>. Accessed 20 Oct 2016</div></div><div class="Citation"><div class="CitationNumber">14.</div><div class="CitationContent" id="CR14">Center for Assured Software. Juliet test suite v1.2 for c/c++ user guide. <span class="ExternalRef"><a href="https://samate.nist.gov/SRD/resources/Juliet_Test_Suite_v1.2_for_C_Cpp_-_User_Guide.pdf"><span class="RefSource">https://​samate.​nist.​gov/​SRD/​resources/​Juliet_​Test_​Suite_​v1.​2_​for_​C_​Cpp_​-_​User_​Guide.​pdf</span></a></span>. Accessed 20 Oct 2016</div></div><div class="Citation"><div class="CitationNumber">15.</div><div class="CitationContent" id="CR15">Goseva-Popstojanova, K., Perhinschi, A.: On the capability of static code analysis to detect security vulnerabilities. Inf. Softw. Technol. <span class="EmphasisTypeBold">68</span>, 18–33 (2015)<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a href="https://doi.org/10.1016/j.infsof.2015.08.002"><span><span>Crossref</span></span></a></span></span></div></div><div class="Citation"><div class="CitationNumber">16.</div><div class="CitationContent" id="CR16">Greenwood, D.J., Levin, M.: Introduction to Action Research: Social Research for Social Change. SAGE Publications, Thousand Oaks (2006)</div></div><div class="Citation"><div class="CitationNumber">17.</div><div class="CitationContent" id="CR17">Hofer, T.: Evaluating static source code analysis tools. Technical report (2010)</div></div><div class="Citation"><div class="CitationNumber">18.</div><div class="CitationContent" id="CR18">Johnson, B., Song, Y., Murphy-Hill, E., Bowdidge, R.: Why don’t software developers use static analysis tools to find bugs? In: 2013 35th International Conference on Software Engineering (ICSE), pp. 672–681. IEEE (2013)</div></div><div class="Citation"><div class="CitationNumber">19.</div><div class="CitationContent" id="CR19">Okun, V., Delaitre, A., Black, P.E.: NIST SAMATE: static analysis tool exposition (sate) iv, March 2012. <span class="ExternalRef"><a href="https://samate.nist.gov/SATE.html"><span class="RefSource">https://​samate.​nist.​gov/​SATE.​html</span></a></span></div></div><div class="Citation"><div class="CitationNumber">20.</div><div class="CitationContent" id="CR20">Oyetoyan, T.D., Soares Cruzes, D., Jaatun, M.G.: An empirical study on the relationship between software security skills, usage and training needs in agile settings. In: 2016 11th International Conference on Availability, Reliability and Security (ARES), pp. 548–555. IEEE (2016)</div></div><div class="Citation"><div class="CitationNumber">21.</div><div class="CitationContent" id="CR21">Phillips, A., Sens, M., de Jonge, A., van Holsteijn, M.: The IT Managers Guide to Continuous Delivery: Delivering Software in Days. BookBaby, Pennsauken (2014)</div></div><div class="Citation"><div class="CitationNumber">22.</div><div class="CitationContent" id="CR22">Rindell, K., Hyrynsalmi, S., Leppänen, V.: Case study of security development in an agile environment: building identity management for a government agency. In: 2016 11th International Conference on Availability, Reliability and Security (ARES), pp. 556–563. IEEE (2016)</div></div><div class="Citation"><div class="CitationNumber">23.</div><div class="CitationContent" id="CR23">Smith, J., Johnson, B., Murphy-Hill, E., Chu, B., Lipford, H.R.: Questions developers ask while diagnosing potential security vulnerabilities with static analysis. In: Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering, pp. 248–259. ACM (2015)</div></div><div class="Citation"><div class="CitationNumber">24.</div><div class="CitationContent" id="CR24">Wagner, A., Sametinger, J.: Using the Juliet test suite to compare static security scanners. In: 2014 11th International Conference on Security and Cryptography (SECRYPT), pp. 1–9. IEEE (2014)</div></div><div class="Citation"><div class="CitationNumber">25.</div><div class="CitationContent" id="CR25">Wäyrynen, J., Bodén, M., Boström, G.: Security engineering and extreme programming: an impossible marriage? In: Zannier, C., Erdogmus, H., Lindstrom, L. (eds.) XP/Agile Universe 2004. LNCS, vol. 3134, pp. 117–128. Springer, Heidelberg (2004). <span class="ExternalRef"><a href="https://doi.org/10.1007/978-3-540-27777-4_12"><span class="RefSource">https://​doi.​org/​10.​1007/​978-3-540-27777-4_​12</span></a></span><span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a href="https://doi.org/10.1007/978-3-540-27777-4_12"><span><span>Crossref</span></span></a></span></span></div></div><div class="Citation"><div class="CitationNumber">26.</div><div class="CitationContent" id="CR26">Zheng, J., Williams, L., Nagappan, N., Snipes, W., Hudepohl, J.P., Vouk, M.A.: On the value of static analysis for fault detection in software. IEEE Trans. Softw. Eng. <span class="EmphasisTypeBold">32</span>(4), 240–253 (2006)<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a href="https://doi.org/10.1109/TSE.2006.38"><span><span>Crossref</span></span></a></span></span></div></div></div></div><div class="FootnoteSection"><div class="Heading">Footnotes</div><div class="Footnote" id="Fn1"><span class="FootnoteNumber"><a href="#Fn1_source">1</a></span><div class="FootnoteParas"><div id="Par4" class="Para"><span class="ExternalRef"><a href="https://cwe.mitre.org/"><span class="RefSource">https://​cwe.​mitre.​org/​</span></a></span>.</div></div><div class="ClearBoth"> </div></div><div class="Footnote" id="Fn2"><span class="FootnoteNumber"><a href="#Fn2_source">2</a></span><div class="FootnoteParas"><div id="Par8" class="Para"><span class="ExternalRef"><a href="http://www.sintef.no/sos-agile"><span class="RefSource">http://​www.​sintef.​no/​sos-agile</span></a></span>.</div></div><div class="ClearBoth"> </div></div><div class="Footnote" id="Fn3"><span class="FootnoteNumber"><a href="#Fn3_source">3</a></span><div class="FootnoteParas"><div id="Par27" class="Para">Bisera Milosheska and Tosin Daniel Oyetoyan. Analyzetoolextended. <span class="ExternalRef"><a href="https://github.com/biseram/AnalyzeToolExtended"><span class="RefSource">https://​github.​com/​biseram/​AnalyzeToolExten​ded</span></a></span>.</div></div><div class="ClearBoth"> </div></div><div class="Footnote" id="Fn4"><span class="FootnoteNumber"><a href="#Fn4_source">4</a></span><div class="FootnoteParas"><div id="Par62" class="Para">Jenkins is a self-contained, open source automation server, which can be used to automate all sorts of tasks such as building, testing, and deploying software.</div></div><div class="ClearBoth"> </div></div></div></div></body></html>
