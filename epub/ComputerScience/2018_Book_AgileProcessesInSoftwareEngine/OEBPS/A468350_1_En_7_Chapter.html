<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/><title>Automated Acceptance Tests as Software Requirements: An Experiment to Compare the Applicability of Fit Tables and Gherkin Language</title><link href="springer_epub.css" type="text/css" rel="styleSheet"/></head><body><div class="ChapterContextInformation"><div class="ContextInformation" id="Chap7"><div class="ChapterCopyright">© The Author(s) 2018</div><span class="ContextInformationAuthorEditorNames"><span class="Editor"><span class="EditorName">Juan Garbajosa</span>, </span><span class="Editor"><span class="EditorName">Xiaofeng Wang</span> and </span><span class="Editor"><span class="EditorName">Ademar Aguiar</span></span><span class="CollaboratorDesignation"> (eds.)</span></span><span class="ContextInformationBookTitles"><span class="BookTitle" xml:lang="en">Agile Processes in Software Engineering and Extreme Programming</span></span><span class="ContextInformationSeries"><span class="SeriesTitle" xml:lang="en">Lecture Notes in Business Information Processing</span><span class="ContextInformationVolumeNumber">314</span></span><span class="ChapterDOI"><a href="A468350_1_En_7_Chapter.html">https://doi.org/10.1007/978-3-319-91602-6_7</a></span></div></div><!--Begin Abstract--><div class="MainTitleSection"><h1 class="ChapterTitle" xml:lang="en">Automated Acceptance Tests as Software Requirements: An Experiment to Compare the Applicability of <span class="EmphasisTypeItalic">Fit Tables</span> and <span class="EmphasisTypeItalic">Gherkin Language</span></h1></div><div class="AuthorGroup"><div class="AuthorNames"><span class="Author"><span class="AuthorName">Ernani César dos Santos</span><sup>1 <a href="#ContactOfAuthor1"><span class="ContactIcon"> </span></a></sup> and </span><span class="Author"><span class="AuthorName">Patrícia Vilain</span><sup>1 <a href="#ContactOfAuthor2"><span class="ContactIcon"> </span></a></sup></span></div><div class="Affiliations"><div class="Affiliation" id="Aff9"><span class="AffiliationNumber">(1)</span><div class="AffiliationText">Federal University of Santa Catarina, Florianópolis, Santa Catarina, Brazil</div></div><div class="ClearBoth"> </div></div><div class="Contacts"><div class="Contact" id="ContactOfAuthor1"><div class="ContactIcon"> </div><div class="ContactAuthorLine"><span class="AuthorName">Ernani César dos Santos</span> (Corresponding author)</div><div class="ContactAdditionalLine"><span class="ContactType">Email: </span><a href="mailto:ernani.santos@posgrad.ufsc.br" class="Email">ernani.santos@posgrad.ufsc.br</a></div></div><div class="Contact" id="ContactOfAuthor2"><div class="ContactIcon"> </div><div class="ContactAuthorLine"><span class="AuthorName">Patrícia Vilain</span> (Corresponding author)</div><div class="ContactAdditionalLine"><span class="ContactType">Email: </span><a href="mailto:patricia.vilain@ufsc.br" class="Email">patricia.vilain@ufsc.br</a></div></div></div></div><div class="Abstract" id="Abs1" xml:lang="en"><div class="Heading">Abstract</div><div id="Par1" class="Para">It is estimated that 85% of the defects in the developed software are originated from ambiguous, incomplete and wishful thinking software requirements. Natural language is often used to write software requirements specifications as well as user requirements. However, natural language specifications can be confusing and hard to understand. Some agile methodologists consider that acceptance tests are more precise and accurate sources of information about the customer’s needs than descriptions in natural language. Several studies have addressed the use of acceptance tests as software requirements specification. Therefore, none of the previous studies has performed experiments to compare the applicability of different acceptance testing techniques in order to support an organization in the selection of one technique over another. This paper addresses this problem reporting an experiment conducted with undergraduate students in Computer Science. This experiment compares the applicability of two acceptance testing techniques (Fit tables and Gherkin language) as software requirements specification. This research tries to answer three questions: (a) Which technique is the easiest to learn in order to specify acceptance test scenarios? (b) Which technique requires less effort to specify acceptance tests? (c) Which technique is the best one to communicate software requirements? The results show that there is no sufficient evidence to affirm that one technique is easier to specify test scenarios or better to communicate software requirements. Whereas, the comparison of effort in terms of time to specify acceptance testing shows that the mean time to specify test scenarios using Gherkin language is lower than Fit tables.
</div></div><div class="KeywordGroup" xml:lang="en"><div class="Heading">Keywords</div><span class="Keyword">Acceptance testing</span><span class="Keyword">Software requirements</span><span class="Keyword">Fit tables</span><span class="Keyword">Gherkin language</span><span class="Keyword">FitNesse</span><span class="Keyword">Cucumber</span><span class="Keyword">TDD</span><span class="Keyword">ATDD</span><span class="Keyword">BDD</span></div><!--End Abstract--><div class="Fulltext"><div id="Sec1" class="Section1 RenderAsSection1"><h2 class="Heading"><span class="HeadingNumber">1 </span>Introduction</h2><div id="Par2" class="Para">Software system functionalities are specified through requirements engineering artifacts, which are a valuable starting point for the software development [<cite><a href="#CR1">1</a></cite>]. Natural language is often used to write system requirements specifications as well as user requirements [<cite><a href="#CR2">2</a></cite>]. According to [<cite><a href="#CR1">1</a></cite>], most of the software requirements specifications are written in natural language.</div><div id="Par3" class="Para">However, natural language specifications can be confusing and hard to understand. Various problems can arise when requirements are written in natural language, for example, readers and writers can use the same word for different concepts, or even, it is possible to express the same concept in completely different ways [<cite><a href="#CR2">2</a></cite>]. In addition, it is estimated that 85% of the defects in the developed software are originated from ambiguous, incomplete, and wishful thinking software requirements [<cite><a href="#CR3">3</a></cite>].</div><div id="Par4" class="Para">Some agile methodologists utilize acceptance tests as a way to specify software requirements [<cite><a href="#CR3">3</a></cite>–<cite><a href="#CR6">6</a></cite>] instead of using more common artifacts based on natural language. They consider that acceptance tests are more precise and accurate sources of information about the customer’s needs than descriptions in natural language [<cite><a href="#CR7">7</a></cite>].</div><div id="Par5" class="Para">Besides the improvement over requirements specification expressed in natural language, acceptance tests also collaborate to the requirements gathering process, because they promote integration between stakeholders and software engineers during the writing of test scenarios of the application to be developed.</div><div id="Par6" class="Para">Several studies have addressed the use of acceptance tests as software requirements specification. However, none of the previous studies has performed experiments using more than one technique to compare the applicability of them as software requirements in the same project in order to support organizations in the selection of one technique over another. This paper addresses this problem reporting an experiment conducted with undergraduate students of the Computer Science program at the Federal University of Santa Catarina to compare the use of a tabular notation for acceptance test scenarios versus a textual scenario notation, which are Fit tables and Gherkin language, respectively.</div><div id="Par7" class="Para">The rest of this paper is organized as follows. Section <span class="InternalRef"><a href="#Sec2">2</a></span> presents the related works. Section <span class="InternalRef"><a href="#Sec3">3</a></span> presents an overview of the main concepts related to this paper. Section <span class="InternalRef"><a href="#Sec4">4</a></span> defines the design of our experimentation and the research questions. In Sect. <span class="InternalRef"><a href="#Sec11">5</a></span> we propose answers for each research question and discuss the results. Section <span class="InternalRef"><a href="#Sec16">6</a></span> presents the threats to the validity. Section <span class="InternalRef"><a href="#Sec17">7</a></span> presents the conclusion and future works.</div></div><div id="Sec2" class="Section1 RenderAsSection1"><h2 class="Heading"><span class="HeadingNumber">2 </span>Related Works</h2><div id="Par8" class="Para">In [<cite><a href="#CR4">4</a></cite>] two experiments were conducted using the tables of the Framework for Integrated Test (Fit). The results show that when software requirements are written in natural language and complemented by Fit tables, they become four times easier to understand by developers than when Fit tables are not used. However, the authors claim that Fit tables do not replace textual requirements, but rather, they suggest that these tables bridge the gaps of software requirements specification which are written exclusively using natural language, reducing the ambiguity and misinterpretation of them.</div><div id="Par9" class="Para">In [<cite><a href="#CR5">5</a></cite>] an experiment with master students was performed. The experiment aims to verify the use of executable Fit acceptance test scenarios as software requirements in maintenance and evolution tasks. The results indicate that Fit tables help developers to perform the maintenance tasks correctly and they also show that these tables may be used to perform regression tests.</div><div id="Par10" class="Para">Melnik et al. [<cite><a href="#CR7">7</a></cite>] have performed an experiment to show that non-technical users, working together with software engineers, can use acceptance test scenarios as a way to communicate and to validate software business requirements. The acceptance testing technique used in this experimentation was the Fit tables. Although the experimentation concludes that non-technical users can specify clearly software requirements using Fit tables, it points out that users have difficulty in learning how to specify test scenarios using this notation. Additionally, this study shows that some non-technical users do not approve the use of Fit tables as an artifact to specify requirements.</div><div id="Par11" class="Para">A user-centered language called BehaviorMap is proposed in [<cite><a href="#CR8">8</a></cite>]. This language is based on behavior models written in Gherkin language that aims to specify behavioral user scenarios in a cognitive way. In this study, an experiment was conducted with 15 individuals to verify the understandability of the BehaviorMap. The results show that BehaviorMap scenarios are easier to understand in relation to textual scenarios, especially when considering scenarios with higher complexity.</div><div id="Par12" class="Para">The use of acceptance test scenarios as an artifact to specify software requirements were also analyzed in [<cite><a href="#CR9">9</a></cite>], which performed an experiment to verify the capability of non-technical users in creating user scenarios of a puzzle game using acceptance testing. The acceptance testing technique used in this experimentation was the User Scenario through User Interaction Diagram (US-UID). The experimentation has pointed out that non-technical users could create US-UID scenarios of the application correctly with a few hours of training.</div><div id="Par13" class="Para">These previous studies have focused on verifying the applicability of acceptance tests as an artifact to clarify software requirements specifications written in natural language or have checked their applicability as software requirements specifications rather than using artifacts such as user stories or use cases. However, none of them compared the applicability of two different notations to express acceptance tests and their adherence to communicate software requirements. This study compares the use of a tabular notation, Fit tables, versus a textual scenario notation, Gherkin language, in terms of: ease of learning, ease of use, effort required to specify acceptance tests scenarios, and capability to communicate software requirements.</div></div><div id="Sec3" class="Section1 RenderAsSection1"><h2 class="Heading"><span class="HeadingNumber">3 </span>Background</h2><div id="Par14" class="Para">Test-driven development (TDD) is a software development approach which tests are written before beginning the development of the SUT, this practice becomes widely established after 1999 by Kent Beck. This practice is performed in five steps, as follows [<cite><a href="#CR13">13</a></cite>]:<div class="OrderedList"><ol><li class="ListItem"><span class="ItemNumber">1.</span><div class="ItemContent"><div><div id="Par15" class="Para">Write a new test case.</div></div></div><div class="ClearBoth"> </div></li><li class="ListItem"><span class="ItemNumber">2.</span><div class="ItemContent"><div><div id="Par16" class="Para">Run all test cases and see the new one fails.</div></div></div><div class="ClearBoth"> </div></li><li class="ListItem"><span class="ItemNumber">3.</span><div class="ItemContent"><div><div id="Par17" class="Para">Write just enough code to make the test pass.</div></div></div><div class="ClearBoth"> </div></li><li class="ListItem"><span class="ItemNumber">4.</span><div class="ItemContent"><div><div id="Par18" class="Para">Re-run the test cases and see them all pass.</div></div></div><div class="ClearBoth"> </div></li><li class="ListItem"><span class="ItemNumber">5.</span><div class="ItemContent"><div><div id="Par19" class="Para">Refactor code to remove duplication.</div></div></div><div class="ClearBoth"> </div></li></ol></div>
</div><div id="Par20" class="Para">In 2002, Ward Cunningham introduced the concept of Fit Tables. In this approach, users write acceptance tests using Fit tables, and programmers write fixtures (glue code) to connect these tables with the future source code of the SUT. The remaining process of this approach is equivalent to steps 2 through 5 of the TDD. This process is called Acceptance test-driven development (ATDD) because acceptance tests are written before the SUT [<cite><a href="#CR14">14</a></cite>].</div><div id="Par21" class="Para">Acceptance testing is a black box testing that aims to determine if a software system meets customer requirements from user’s point of view [<cite><a href="#CR3">3</a></cite>, <cite><a href="#CR7">7</a></cite>, <cite><a href="#CR9">9</a></cite>]. As defined in the IEEE Standard 1012-1986 [<cite><a href="#CR10">10</a></cite>], acceptance testing is a “formal testing conducted to determine whether or not a system satisfies its acceptance criteria and to enable the customer to determine whether or not to accept the system”.</div><div id="Par22" class="Para">Fit is an example of framework to express acceptance test scenarios. Using this framework, the acceptance tests are written in the form of tables, which are called Fit tables. Besides Fit tables are used to represent test scenarios, they are also used for reporting the results of tests [<cite><a href="#CR11">11</a></cite>]. Figure <span class="InternalRef"><a href="#Fig1">1</a></span> shows an example of Fit report table, which was used to perform several tests in a functionality to calculate discount over an amount. The first column of this table, named amount, represents an input, whereas, the second columns, which name is followed by parenthesis, represent the expected output. When a test fails, the expected and actual output values are showed to the user [<cite><a href="#CR11">11</a></cite>].<div class="Figure" id="Fig1"><div class="MediaObject" id="MO1"><img src="A468350_1_En_7_Fig1_HTML.gif" alt="A468350_1_En_7_Fig1_HTML.gif"/></div><div class="Caption" xml:lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 1.</span><div class="SimplePara">Fit table report sample [<cite><a href="#CR11">11</a></cite>].</div></div></div></div>
</div><div id="Par23" class="Para">Behavior-driven development (BDD) is an agile software development approach that enhances the paradigm of TDD for acceptance testing. In the BDD approach, the behavior of the SUT is described through user stories and acceptance tests before beginning its development. Scenarios representing the user stories are described using BDD languages, such as Gherkin language [<cite><a href="#CR12">12</a></cite>].</div><div id="Par24" class="Para">Gherkin language is a domain specific language (DSL) that can express the behavior and the expected outcome of the SUT [<cite><a href="#CR12">12</a></cite>]. It uses some words as commands, such as Given, When and Then. The word <span class="EmphasisTypeItalic">Given</span> expresses the inputs or pre-conditions to perform a test, the word <span class="EmphasisTypeItalic">When</span> expresses conditions or specified behavior, and the word <span class="EmphasisTypeItalic">Then</span> expresses expected outputs or expected changes due to the specified behavior. As with the Fit tables, Gherkin language also needs a glue code to connect the features (a set of test scenarios) with the source code of the SUT [<cite><a href="#CR12">12</a></cite>]. An example of the syntax of this language is as follows:<div class="UnorderedList"><ul class="UnorderedListMarkNone"><li><div id="Par25" class="Para">
                      <span class="Literal">
                        <span class="EmphasisTypeBold"><span class="EmphasisFontCategoryNonProportional">Scenario</span></span>
                      </span>
                      <span class="Literal">
                        <span class="EmphasisFontCategoryNonProportional">: Pop element</span>
                      </span>
                    </div></li><li><div id="Par26" class="Para">
                      <span class="Literal">
                        <span class="EmphasisTypeBold"><span class="EmphasisFontCategoryNonProportional">Given</span></span>
                      </span>
                      <span class="Literal">
                        <span class="EmphasisFontCategoryNonProportional">a non-empty Stack</span>
                      </span>
                    </div></li><li><div id="Par27" class="Para">
                      <span class="Literal">
                        <span class="EmphasisTypeBold"><span class="EmphasisFontCategoryNonProportional">When</span></span>
                      </span>
                      <span class="Literal">
                        <span class="EmphasisFontCategoryNonProportional">a stack has N elements</span>
                      </span>
                    </div></li><li><div id="Par28" class="Para">
                      <span class="Literal">
                        <span class="EmphasisTypeBold"><span class="EmphasisFontCategoryNonProportional">And</span></span>
                      </span>
                      <span class="Literal">
                        <span class="EmphasisFontCategoryNonProportional">element E is on top of the stack</span>
                      </span>
                    </div></li><li><div id="Par29" class="Para">
                      <span class="Literal">
                        <span class="EmphasisTypeBold"><span class="EmphasisFontCategoryNonProportional">Then</span></span>
                      </span>
                      <span class="Literal">
                        <span class="EmphasisFontCategoryNonProportional">a pop operation returns E</span>
                      </span>
                    </div></li><li><div id="Par30" class="Para">
                      <span class="Literal">
                        <span class="EmphasisTypeBold"><span class="EmphasisFontCategoryNonProportional">And</span></span>
                      </span>
                      <span class="Literal">
                        <span class="EmphasisFontCategoryNonProportional">the new size of the stack is N-1</span>
                      </span>
                    </div></li></ul></div>
</div></div><div id="Sec4" class="Section1 RenderAsSection1"><h2 class="Heading"><span class="HeadingNumber">4 </span>Experiment Definition</h2><div id="Par31" class="Para">In this section, we report the experiment definition, design, and planning, following the guidelines proposed in [<cite><a href="#CR15">15</a></cite>, <cite><a href="#CR16">16</a></cite>], as well the experiments conducted in [<cite><a href="#CR3">3</a></cite>, <cite><a href="#CR4">4</a></cite>]. The objective of this experimentation is to compare the applicability of Fit tables and Gherkin language to communicate requirements in a software development process regarding specification effort and requirements consistency. The perspective is to adopt Fit tables or Gherkin language to express software system requirements in outsourcing contracts for software development. The context of the experiment consists of undergraduate students (subjects) and a Java application (object). The participants (subjects) involved in the experiment are undergraduate students in the last years of the Computer Science program. The object of this study is a human resource (HR) management application named HRS, which supports functionalities such as compliance, payroll, personnel files, and benefits administration.</div><div id="Sec5" class="Section2 RenderAsSection2"><h3 class="Heading"><span class="HeadingNumber">4.1 </span>Experiment Participants</h3><div id="Par32" class="Para">The participants were 18 students from a course called Special Topics in Technology Applications I, in the last years of the bachelor’s degree in Computer Science at UFSC. The students have already attended courses on software programming and software engineering, and they had a medium knowledge and expertise level in programming and software engineering topics. The most of them have been taking part of trainee programs. The participants have never taken any course or professional experience in Fit or Gherkin language. Although the experiment was conducted as a mandatory activity of the course, the students were not graded based on the artifacts produced, but rather, they were graded based on their participation. Also, students were advised that the activities were parts of an experiment to compare the applicability of two acceptance testing techniques as artifacts to communicate software requirements.</div></div><div id="Sec6" class="Section2 RenderAsSection2"><h3 class="Heading"><span class="HeadingNumber">4.2 </span>Experiment Material</h3><div id="Par33" class="Para">The experiment was performed on a set of four requirements for the HRS application. We granted access permission for each participant to access a web directory that contains a textual description of the application, instructions to set up the application project (download the zipped Java project and import it into the Eclipse IDE), a time sheet and a questionnaire. The timesheet was used by the participants to take note of the time spent in each task of the experiment. The questionnaire is a set of 24 questions to investigate the background of the participants and to perform a qualitative analysis of the performed tasks. The answers for these questions are five-point scales, such as 1 = Strongly agree, 2 = Agree, 3 = Not certain, 4 = Disagree, 5 = Strongly disagree.</div><div id="Par34" class="Para">The development environment was set up by the participants, who received a tutorial to guide this activity. The tutorial content is:<div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><div id="Par35" class="Para">Installation and configuration of Java Enterprise;</div></li><li><div id="Par36" class="Para">Installation and configuration of the standalone version of Fit wiki and its dependencies;</div></li><li><div id="Par37" class="Para">Installation and configuration of Eclipse IDE with the Cucumber plugin;</div></li><li><div id="Par38" class="Para">Quick-start examples to validate all development environments.</div></li></ul></div>
</div></div><div id="Sec7" class="Section2 RenderAsSection2"><h3 class="Heading"><span class="HeadingNumber">4.3 </span>Hypothesis Definition</h3><div id="Par39" class="Para">Considering Fit tables and Gherkin language as available acceptance testing techniques, this experiment addresses the following research questions:<div class="UnorderedList"><ul class="UnorderedListMarkDash"><li><div id="Par40" class="Para ParaOneEmphasisChild"><span class="EmphasisTypeBold">RQ1.</span> Which of these acceptance tests techniques is easier to learn?</div></li><li><div id="Par41" class="Para ParaOneEmphasisChild"><span class="EmphasisTypeBold">RQ2</span>. Which of these acceptance tests techniques requires less effort (in time) to specify acceptance test scenarios?</div></li><li><div id="Par42" class="Para ParaOneEmphasisChild"><span class="EmphasisTypeBold">RQ3</span>. Which of these acceptance tests techniques is the best one to communicate software requirements, as a form of expressing consistent requirements?</div></li></ul></div>
</div><div id="Par43" class="Para">Once the research questions are formulated, it is possible to turn it into null hypotheses to be tested in the experiment:<div class="UnorderedList"><ul class="UnorderedListMarkDash"><li><div id="Par44" class="Para"><span class="EmphasisTypeBold">H</span><sub>
                          <span class="EmphasisTypeBold">0a</span>
                        </sub> the correctness of acceptance test scenarios specified by participants who attended a three-hour lecture about Fit tables and Gherkin language is the same for both acceptance testing techniques.</div></li><li><div id="Par45" class="Para"><span class="EmphasisTypeBold">H</span><sub>
                          <span class="EmphasisTypeBold">0b</span>
                        </sub> the effort to specify acceptance test scenarios is the same for both techniques.</div></li><li><div id="Par46" class="Para"><span class="EmphasisTypeBold">H</span><sub>
                          <span class="EmphasisTypeBold">0c</span>
                        </sub> the correctness of software functionalities specified using Fit tables and Gherkin language and implemented by the participants is the same using both acceptance testing techniques.</div></li></ul></div>
</div><div id="Par47" class="Para">On the other hand, the alternative hypotheses are:<div class="UnorderedList"><ul class="UnorderedListMarkDash"><li><div id="Par48" class="Para"><span class="EmphasisTypeBold">H</span><sub>
                          <span class="EmphasisTypeBold">1a</span>
                        </sub> the correctness of acceptance test scenarios specified by participants who attended a three-hour lecture about Fit tables and Gherkin language is different when both acceptance testing techniques are used.</div></li><li><div id="Par49" class="Para"><span class="EmphasisTypeBold">H</span><sub>
                          <span class="EmphasisTypeBold">1b</span>
                        </sub> the effort to specify acceptance test scenarios using Fit tables is not the same when Gherkin language is used.</div></li><li><div id="Par50" class="Para"><span class="EmphasisTypeBold">H</span><sub>
                          <span class="EmphasisTypeBold">1c</span>
                        </sub> the correctness of software functionalities implemented by the participants is different when both acceptance testing techniques are used.</div></li></ul></div>
</div><div id="Par51" class="Para">The dependent variables of our study are:<div class="UnorderedList"><ul class="UnorderedListMarkDash"><li><div id="Par52" class="Para ParaOneEmphasisChild"><span class="EmphasisTypeBold">ATS#.</span> Acceptance tests of requirement # were specified: {correctly, incorrectly};</div></li><li><div id="Par53" class="Para ParaOneEmphasisChild"><span class="EmphasisTypeBold">ATST#.</span> The participants need {?} minutes to specify acceptance test scenarios of requirement #;</div></li><li><div id="Par54" class="Para ParaOneEmphasisChild"><span class="EmphasisTypeBold">AR#.</span> The delivered source code implemented based on the acceptance test scenarios of requirement # is executable and it was accepted by the business stakeholder: {yes, no};</div></li></ul></div>
</div><div id="Par55" class="Para">Where the symbol “#” represents a change software requirement identified from SR1 to SR4 in Table <span class="InternalRef"><a href="#Tab1">1</a></span>, and the symbol “?” represents an integer value.<div id="Tab1" class="Table"><div class="Caption" xml:lang="en"><div class="CaptionContent"><span class="CaptionNumber">Table 1.</span><div class="SimplePara">Objects of the experiment</div></div></div><table border="1"><colgroup><col align="left"/><col align="left"/></colgroup><thead><tr class="header"><th align="left"><div class="SimplePara">Id</div></th><th align="left"><div class="SimplePara">Requirement</div></th></tr></thead><tbody><tr class="noclass"><td align="left"><div class="SimplePara">SR1</div></td><td align="left"><div class="SimplePara">Rectification of personnel profile information</div></td></tr><tr class="noclass"><td align="left"><div class="SimplePara">SR2</div></td><td align="left"><div class="SimplePara">Calculation of salary bonus per person</div></td></tr><tr class="noclass"><td align="left"><div class="SimplePara">SR3</div></td><td align="left"><div class="SimplePara">Exclusion of personnel profile information</div></td></tr><tr class="noclass"><td align="left"><div class="SimplePara">SR4</div></td><td align="left"><div class="SimplePara">Calculation the average of salary bonus per position</div></td></tr></tbody></table></div>
</div><div id="Par56" class="Para">The number of acceptance test scenarios that were specified correctly (<span class="EmphasisTypeItalic">ATS</span>#) was obtained from the evaluation of artifacts delivery by participants. This evaluation was performed by a researcher who is expert in acceptance tests and he does not have any connection with the experimentation. The time needed to specify acceptance test scenarios of each requirement (<span class="EmphasisTypeItalic">ATST</span>#) has been measured by asking participants to fill it in the timesheet. The number of requirements correctly coded (<span class="EmphasisTypeItalic">AR</span>#) was obtained from the evaluation of executable source code delivered by participants. This evaluation was conducted by a business stakeholder, who accepted or not the delivered functionality through black box testing. If the business stakeholder accepts the delivered functionality, the coded requirement is considered correct. Otherwise, it is considered incorrect. This person has not been involved with the specification of acceptance test scenarios that were used by participants to develop the set of required software changes.</div></div><div id="Sec8" class="Section2 RenderAsSection2"><h3 class="Heading"><span class="HeadingNumber">4.4 </span>Experiment Design</h3><div id="Par57" class="Para">We divided the experiment into two parts. Part 1 addresses the specification of acceptance test scenarios. Part 2 addresses the implementation of new requirements for the HRS application using the acceptance test scenarios to represent the requirements.</div><div id="Par58" class="Para">In both parts, we have four objects and two treatments. The objects are the new requirements of the HRS application, as shown in Table <span class="InternalRef"><a href="#Tab2">2</a></span>. The treatments are the following:<div id="Tab2" class="Table"><div class="Caption" xml:lang="en"><div class="CaptionContent"><span class="CaptionNumber">Table 2.</span><div class="SimplePara">Experiment design of Part 1 – Specification of acceptance test scenarios. At the top of this table, SR1, SR2, SR3, and SR4 are abbreviations to the objects listed in Table <span class="InternalRef"><a href="#Tab1">1</a></span>.</div></div></div><table border="1"><colgroup><col align="left"/><col align="left"/><col align="left"/><col align="left"/><col align="left"/></colgroup><thead><tr class="header"><th rowspan="2" align="left"><div class="SimplePara">Participants</div></th><th colspan="4" align="left"><div class="SimplePara">Objects and treatments</div></th></tr><tr class="header"><th align="left"><div class="SimplePara">
                              <span class="EmphasisTypeItalic">SR1</span>
                            </div></th><th align="left"><div class="SimplePara">
                              <span class="EmphasisTypeItalic">SR2</span>
                            </div></th><th align="left"><div class="SimplePara">
                              <span class="EmphasisTypeItalic">SR3</span>
                            </div></th><th align="left"><div class="SimplePara">
                              <span class="EmphasisTypeItalic">SR4</span>
                            </div></th></tr></thead><tbody><tr class="noclass"><td align="left"><div class="SimplePara">Group A</div></td><td align="left"><div class="SimplePara">(F)</div></td><td align="left"><div class="SimplePara">(F)</div></td><td align="left"><div class="SimplePara">–</div></td><td align="left"><div class="SimplePara">–</div></td></tr><tr class="noclass"><td align="left"><div class="SimplePara">Group B</div></td><td align="left"><div class="SimplePara">–</div></td><td align="left"><div class="SimplePara">–</div></td><td align="left"><div class="SimplePara">(G)</div></td><td align="left"><div class="SimplePara">(G)</div></td></tr></tbody></table></div>
<div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><div id="Par59" class="Para ParaOneEmphasisChild"><span class="EmphasisTypeBold">(F)</span> Software requirements specified as acceptance test scenarios using Fit Tables.</div></li><li><div id="Par60" class="Para ParaOneEmphasisChild"><span class="EmphasisTypeBold">(G)</span> Software requirements specified as acceptance test scenarios using Gherkin language.</div></li></ul></div>
</div><div id="Par61" class="Para">The participants were split into two groups, which were identified by the letters A and B. In Part 1, the group A specified two software requirements as acceptance test scenarios using Fit tables, meanwhile, the group B specified two software requirements as acceptance test scenarios using Gherkin language. Table <span class="InternalRef"><a href="#Tab2">2</a></span> shows, for each group, which treatment was used to specify which software requirement.</div><div id="Par62" class="Para">In Part 2 of this experiment, the set of software requirements specified by group A were send to group B, and vice versa. Then, as shown in Table <span class="InternalRef"><a href="#Tab3">3</a></span>, the group A developed requirements SR3 and SR4, which were specified by group B using Gherkin language, whereas, the group B developed requirements SR1 and SR2, which were specified by group A using Fit tables. Before performing this exchange of acceptance test scenarios between the groups, an expert verified the correctness and conciseness of each scenario. Test scenarios that presented problems were replaced by others, which were correct and express the same set of requirements. This intervention was necessary to prevent false negatives in the analysis of capability of executable acceptance tests to communicate software requirements.<div id="Tab3" class="Table"><div class="Caption" xml:lang="en"><div class="CaptionContent"><span class="CaptionNumber">Table 3.</span><div class="SimplePara">Experiment design of Part 2 – implementation of new requirements.</div></div></div><table border="1"><colgroup><col align="left"/><col align="left"/><col align="left"/><col align="left"/><col align="left"/></colgroup><thead><tr class="header"><th rowspan="2" align="left"><div class="SimplePara">Participants</div></th><th colspan="4" align="left"><div class="SimplePara">Objects and treatments</div></th></tr><tr class="header"><th align="left"><div class="SimplePara">
                              <span class="EmphasisTypeItalic">SR1</span>
                            </div></th><th align="left"><div class="SimplePara">
                              <span class="EmphasisTypeItalic">SR2</span>
                            </div></th><th align="left"><div class="SimplePara">
                              <span class="EmphasisTypeItalic">SR3</span>
                            </div></th><th align="left"><div class="SimplePara">
                              <span class="EmphasisTypeItalic">SR4</span>
                            </div></th></tr></thead><tbody><tr class="noclass"><td align="left"><div class="SimplePara">Group A</div></td><td align="left"><div class="SimplePara">–</div></td><td align="left"><div class="SimplePara">–</div></td><td align="left"><div class="SimplePara">(G)</div></td><td align="left"><div class="SimplePara">(G)</div></td></tr><tr class="noclass"><td align="left"><div class="SimplePara">Group B</div></td><td align="left"><div class="SimplePara">(F)</div></td><td align="left"><div class="SimplePara">(F)</div></td><td align="left"><div class="SimplePara">–</div></td><td align="left"><div class="SimplePara">–</div></td></tr></tbody></table></div>
</div></div><div id="Sec9" class="Section2 RenderAsSection2"><h3 class="Heading"><span class="HeadingNumber">4.5 </span>Training</h3><div id="Par63" class="Para">Participants have been trained in meaning and usage of the following subjects:<div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><div id="Par64" class="Para">a half-hour lecture about acceptance testing, TDD, ATDD, and BDD;</div></li><li><div id="Par65" class="Para">one-and-a-half-hour lecture about Fit tables and FitNesse, including how to configure this framework and practice exercises;</div></li><li><div id="Par66" class="Para">one-and-a-half-hour lecture about Gherkin language and Cucumber, including how to configure this framework and practice exercises.</div></li></ul></div>
</div></div><div id="Sec10" class="Section2 RenderAsSection2"><h3 class="Heading"><span class="HeadingNumber">4.6 </span>Experiment Procedure</h3><div id="Par67" class="Para">The experimentation was carried out as explained in the following. First, the participants were given a short introduction to the experimentation, then they were randomly assigned to one of the two groups. After this, they received the timesheet and the questionnaire, described in Sect. <span class="InternalRef"><a href="#Sec6">4.2</a></span>.</div><div id="Par68" class="Para">Then, the experiment was conduct according to the following steps:<div class="UnorderedList"><ul class="UnorderedListMarkDash"><li><div id="Par69" class="Para">(1) Participants had 20 min to check if their environment to specify acceptance tests, which was previously set up in the training section, was working. In this step, they also answered the six first questions of the questionnaire.</div></li><li><div id="Par70" class="Para">(2) Participants read an overview of the HRS application and received a document with the description of two new requirements for this application.</div></li><li><div id="Par71" class="Para">(3) For each requirement:</div></li><li><div id="Par72" class="Para">(3.a) Participants filled the start time in their time sheets.</div></li><li><div id="Par73" class="Para">(3.b) Participants had to understand the requirements; if they had any doubt a business stakeholder was available to clarify them.</div></li><li><div id="Par74" class="Para">(3.c) Participants had to specify the requirement using the acceptance testing technique assign to them.</div></li><li><div id="Par75" class="Para">(3.d) When finished, participants had to mark the stop time on their time sheet.</div></li><li><div id="Par76" class="Para">(4) Then, participants had to answer the next eight questions of the questionnaire and to send the produced artifact to a web repository. The artifacts were identified by a random numeric id and only the researchers knew who uploaded them.</div></li><li><div id="Par77" class="Para">(5) An expert in acceptance testing evaluated all uploaded artifacts and marked the ones that were inconsistent or incomplete. This mark was visible only to the researchers.</div></li><li><div id="Par78" class="Para">(6) In the sixth step, the second part of our experiment was started. Participants had 20 min to check if their environment to develop the next tasks was working. After this, they had to download acceptance tests artifacts produced by a participant of the other group.</div></li><li><div id="Par79" class="Para">(7) Then, participants had to answer two questions in the questionnaire related to their view about the downloaded artifacts.</div></li><li><div id="Par80" class="Para">(8) The researchers had to verify which participants downloaded acceptance tests that, according to the expert evaluation, were incorrect. Then, they exchanged the incorrect acceptance tests by correct tests that express the same requirements using the same acceptance testing technique.</div></li><li><div id="Par81" class="Para">(9) Then, for each acceptance test scenario (requirement):</div></li><li><div id="Par82" class="Para">(9.a) Participants had to fill the start time in their time sheets.</div></li><li><div id="Par83" class="Para">(9.b) Participants had to understand by themselves the acceptance test.</div></li><li><div id="Par84" class="Para">(9.c) Participants had to develop the new requirement of the HRS application expressed by the acceptance tests.</div></li><li><div id="Par85" class="Para">(9.d) When finished, participants had to mark the stop time on their time sheet.</div></li><li><div id="Par86" class="Para">(10) Then, participants had to answer the next eight questions of the questionnaire and to send the produced source code to a web repository. The artifacts were identified by a random numeric id, and only researchers knew who uploaded them.</div></li></ul></div>
</div><div id="Par87" class="Para">This procedure was carried out in two sections. The first with three and a half-hour of duration and the second one with two hours of duration.</div></div></div><div id="Sec11" class="Section1 RenderAsSection1"><h2 class="Heading"><span class="HeadingNumber">5 </span>Results and Data Analysis</h2><div id="Par88" class="Para">In this Section, we show and discuss the results achieved from the experiment. The experiment data and charts are available at <span class="ExternalRef"><a href="http://www.leb.inf.ufsc.br/index.php/xp2018/"><span class="RefSource">www.​leb.​inf.​ufsc.​br/​index.​php/​xp2018/​</span></a></span>.</div><div id="Sec12" class="Section2 RenderAsSection2"><h3 class="Heading"><span class="HeadingNumber">5.1 </span>Consistency and Correctness of the Acceptance Test Scenarios</h3><div id="Par89" class="Para">Table <span class="InternalRef"><a href="#Tab4">4</a></span> is the contingency table for the dependents variables (see Sect. <span class="InternalRef"><a href="#Sec7">4.3</a></span>) from <span class="EmphasisTypeBold">ATS</span><span class="EmphasisTypeItalic">SR1</span> to <span class="EmphasisTypeBold">ATS</span><span class="EmphasisTypeItalic">SR4</span>. The first line of this table shows the number of tasks performed using Fit tables as acceptance testing technique: 17 tasks were completed, and only one task failed. The second line shows the number of tasks performed using Gherkin language as acceptance testing technique: 13 tasks were completed, and five tasks failed. The tasks performed by the same participant were considered as independent measures.<div id="Tab4" class="Table"><div class="Caption" xml:lang="en"><div class="CaptionContent"><span class="CaptionNumber">Table 4.</span><div class="SimplePara">Contingency table for correct specifications of acceptance tests</div></div></div><table border="1"><colgroup><col align="left"/><col align="left"/><col align="left"/></colgroup><thead><tr class="header"><th align="left"> </th><th colspan="2" align="left"><div class="SimplePara">Acceptance test scenarios were specified:</div></th></tr><tr class="header"><th align="left"><div class="SimplePara">Treatment</div></th><th align="left"><div class="SimplePara">Correctly</div></th><th align="left"><div class="SimplePara">Incorrectly</div></th></tr></thead><tbody><tr class="noclass"><td align="left"><div class="SimplePara">Fit Tables (T)</div></td><td align="left"><div class="SimplePara">17</div></td><td align="left"><div class="SimplePara">1</div></td></tr><tr class="noclass"><td align="left"><div class="SimplePara">Gherkin Language (G)</div></td><td align="left"><div class="SimplePara">13</div></td><td align="left"><div class="SimplePara">5</div></td></tr></tbody></table></div>
</div><div id="Par90" class="Para">We applied the Fisher’s test in the data presented in Table <span class="InternalRef"><a href="#Tab4">4</a></span>. This test returned a p-value of 0.1774. The result is not significant at p &lt; 0.05. Thus, <span class="EmphasisTypeBold">H</span><sub>
                    <span class="EmphasisTypeBold">0a</span>
                  </sub> is accepted, there is no statistically significant influence of the treatment on the acceptance test scenarios specification.</div><div id="Par91" class="Para">Although we cannot obtain the answer of RQ1 with Fisher’s test, we suppose that extra training sections and practical exercises could decrease the number of incorrect specification around zero because the errors identified in the test scenarios specified by participants in both techniques are basic mistakes. Thus, we found that the complexity to learn both acceptance testing techniques by software developers is the same.</div></div><div id="Sec13" class="Section2 RenderAsSection2"><h3 class="Heading"><span class="HeadingNumber">5.2 </span>Time to Complete Acceptance Test Scenarios Specifications</h3><div id="Par92" class="Para">Table <span class="InternalRef"><a href="#Tab5">5</a></span> presents the time, in minutes, spent by participants to complete the specification task of each requirement. The underlined time values in this table refer to test scenarios that were specified incorrectly by participants. The tasks performed by the same participant were considered as independent measures, and the distribution of requirements and treatments were conducted as shown in Table <span class="InternalRef"><a href="#Tab2">2</a></span>.<div id="Tab5" class="Table"><div class="Caption" xml:lang="en"><div class="CaptionContent"><span class="CaptionNumber">Table 5.</span><div class="SimplePara">A list of time spent to develop new software requirements in the HRS application.</div></div></div><table border="1"><colgroup><col align="left"/><col align="left"/></colgroup><thead><tr class="header"><th align="left"><div class="SimplePara">Treatment</div></th><th align="left"><div class="SimplePara">Time list (in minutes)</div></th></tr></thead><tbody><tr class="noclass"><td align="left"><div class="SimplePara">Fit tables (F)</div></td><td align="left"><div class="SimplePara">{20, <span class="EmphasisTypeUnderline">21</span>, 21, 23, 35, 36, 40, 45, 50, 65, 66, 77, 79, 88, 108, 120, 126, 135}</div></td></tr><tr class="noclass"><td align="left"><div class="SimplePara">Gherkin Language (G)</div></td><td align="left"><div class="SimplePara">{15, 16, 17, 15, <span class="EmphasisTypeUnderline">39</span>, <span class="EmphasisTypeUnderline">30</span>, 30, 30, 45, 35, <span class="EmphasisTypeUnderline">60</span>, 28, <span class="EmphasisTypeUnderline">40</span>, <span class="EmphasisTypeUnderline">40</span>, 70, 57, 84, 75}</div></td></tr></tbody></table></div>
</div><div id="Par93" class="Para">We used the Shapiro-Wilk normality test to check if the data collected from the experiment for the two treatments have a normal distribution. Then, we performed a t-test that returned a p-value of 0.0291. We also performed the same test excluding the underlined values and we obtained a p-value of 0.0334. The results are significant at p &lt; 0.05. Thus, in both tests <span class="EmphasisTypeBold">H</span><sub>
                    <span class="EmphasisTypeBold">0b</span>
                  </sub> is rejected and the alternative hypotheses are accepted. Therefore, there is a difference, in terms of the mean time spent to specify acceptance tests, between Fit tables and Gherkin language.</div><div id="Par94" class="Para">Answering the RQ2, we found that the effort, in terms of the meantime, to specify acceptance tests using Gherkin language (40 min) is lower than using Fit Tables (64 min).</div></div><div id="Sec14" class="Section2 RenderAsSection2"><h3 class="Heading"><span class="HeadingNumber">5.3 </span>Applicability of Acceptance Test Scenarios to Communicate Software Requirements</h3><div id="Par95" class="Para">Table <span class="InternalRef"><a href="#Tab6">6</a></span> is the contingency table for the dependents variables (see Sect. <span class="InternalRef"><a href="#Sec7">4.3</a></span>) from ARSR1 to ARSR4. The first line of this table shows the number of tasks implemented based on the requirements specified using Fit tables: 13 tasks were successfully completed, and five tasks failed. The second line shows the number of tasks using Gherkin Language: 10 tasks were successfully completed, and eight tasks failed.<div id="Tab6" class="Table"><div class="Caption" xml:lang="en"><div class="CaptionContent"><span class="CaptionNumber">Table 6.</span><div class="SimplePara">Contingency table for correct development of software requirements expressed by acceptance tests</div></div></div><table border="1"><colgroup><col align="left"/><col align="left"/><col align="left"/></colgroup><thead><tr class="header"><th align="left"> </th><th colspan="2" align="left"><div class="SimplePara">The delivered source code implemented based on the acceptance test scenarios is executable and it was accepted by the business stakeholder:</div></th></tr><tr class="header"><th align="left"><div class="SimplePara">Treatment</div></th><th align="left"><div class="SimplePara">Yes</div></th><th align="left"><div class="SimplePara">No</div></th></tr></thead><tbody><tr class="noclass"><td align="left"><div class="SimplePara">Fit Tables (T)</div></td><td align="left"><div class="SimplePara">13</div></td><td align="left"><div class="SimplePara">5</div></td></tr><tr class="noclass"><td align="left"><div class="SimplePara">Gherkin Language (G)</div></td><td align="left"><div class="SimplePara">10</div></td><td align="left"><div class="SimplePara">8</div></td></tr></tbody></table></div>
</div><div id="Par96" class="Para">We applied the Fisher’s test in the data presented in Table <span class="InternalRef"><a href="#Tab6">6</a></span>. This test returned a p-value of 0.4887. The result is not significant at p &lt; 0.05. Thus, H0c is accepted, therefore, there is no statistically significant influence of the treatment on the development of software requirements expressed by acceptance test scenarios using both techniques.</div><div id="Par97" class="Para">Then, addressing the RQ3, we cannot assume based on the Fisher’s test result that a technique is better than another to communicate requirements. In addition, in the same way that we suppose that extra training could improve acceptance test scenarios specification, it also could improve requirements communication. However, despite these experimentation evidences, we claim that Gherkin language scenarios communicate requirements better than Fit tables because we observe that tables are weak in details and depending on the software requirement a complementary textual description is required to communicate a requirement completely, whereas, in Gherkin language the acceptance test scenarios are complemented on default by a textual description.</div></div><div id="Sec15" class="Section2 RenderAsSection2"><h3 class="Heading"><span class="HeadingNumber">5.4 </span>Experiment Questionnaire</h3><div id="Par98" class="Para">In this section, we discuss six questions of the questionnaire that we applied in the experiment. The questions were answered on a five-point scale, where one maps to Strongly agree, two maps to Agree, three maps to Not certain, four maps to Disagree, and five maps to Strongly disagree. Question 1 (Q1) and question 3 (Q3) were applied to group A, whereas question 2 (Q2) and question 4 (Q4) were applied to group B. Questions 5 (Q5) and 6 (Q6) were applied to both groups.</div><div id="Par99" class="Para ParaOneEmphasisChild"><span class="EmphasisTypeBold">Q1. I experienced no difficulty in specifying acceptance test scenarios using Fit tables.</span> Half of the participants strongly agree (22.22%) or agree (27.78%) with this statement and 22.22% are not certain, whereas, the rest of the participants disagree (5.56%) or strongly disagree (22.22%).</div><div id="Par100" class="Para">Although the results presented in Sect. <span class="InternalRef"><a href="#Sec12">5.1</a></span> shows that the major part of the acceptance tests was specified correctly, we observed, through this questionnaire, that participants had difficulty to specify the acceptance tests. So, we realized that in a next experiment we should dedicate more time performing training lectures intending to decrease the time spent by participants to specify acceptance tests and to increase the quality of acceptance test scenarios using Fit tables.</div><div id="Par101" class="Para ParaOneEmphasisChild"><span class="EmphasisTypeBold">Q2. I experienced no difficulty in specifying acceptance test scenarios using Gherkin language.</span> This result was different than we expected. The percentage of participants who answered that they strongly disagree (16.67%) or disagree (16.67%) with this statement is greater than the percentage of who answered that strongly agree (5.56%) or agree (16.67%). The rest of participants (44.44%) are not certain about this statement.</div><div id="Par102" class="Para">Our initial belief was that participants who used Gherkin language had experimented less difficulty to create acceptance test scenarios than ones that used Fit tables because Gherkin language is similar to English spoken language. However, the results obtained from the questionnaire tend to be the opposite. As concluded in Q1, the participants should spend more time with the lecture and exercises to improve their experience with Gherkin Language.</div><div id="Par103" class="Para ParaOneEmphasisChild"><span class="EmphasisTypeBold">Q3. I experienced no difficulty in implementing new requirements in the HRS application, which specification were expressed as acceptance test scenarios writing through Fit tables.</span> The major part of participants reported that they had difficulty in the implementation tasks, 55.56% answered that strongly disagree (27.78%) and disagree (27.78%) with this assertion, whereas 38.89% of the participants are not certain, 5.56% agree and 0.00% strongly agree with this assertion.</div><div id="Par104" class="Para ParaOneEmphasisChild"><span class="EmphasisTypeBold">Q4. I experienced no difficulty in implementing new requirements in the HRS application, which specification were expressed as acceptance test scenarios writing through Gherkin Language.</span> The major part of participants reported that they had difficulty in the implementation tasks, 27.78% and 33.33% answered respectively that strongly disagree and disagree with this assertion, whereas, 27.78% of the participants are not certain, 5.56% agree and 5.56% strongly agree with this assertion.</div><div id="Par105" class="Para">Although only a few participants agree or strongly agree with this assertion, the percentage is two times bigger than the percentage of the same group in Q3. We assign this to the fact that acceptance test scenarios written in Gherkin language are more verbose than tables, which becomes Gherkin language easier to understand than Fit tables. However, as presented in Sect. <span class="InternalRef"><a href="#Sec14">5.3</a></span>, there is no evidence that one technique is better than another to communicate software requirements.</div><div id="Par106" class="Para ParaOneEmphasisChild"><span class="EmphasisTypeBold">Q5. I will use acceptance testing to validate software in future projects.</span> The number of participants (27.78%) that agree with this assertion is greater than the number of participants that disagree (11.11%) or strongly disagree (11.11%). However, 50.00% are in doubt about using acceptance testing to validate software.</div><div id="Par107" class="Para ParaOneEmphasisChild"><span class="EmphasisTypeBold">Q6. I will use acceptance test scenarios to communicate requirements for future projects.</span> 33.33% of the participants did not approve the use of acceptance tests as requirements and they would not like to take part in projects that use this approach. 38.89% of the participants are not certain about this assertion, the rest of the participants, 27.78%, agree with this assertion.</div><div id="Par108" class="Para">In both Q5 and Q6, the number of participants that are in doubt about using acceptance testing to validate software or to communicate requirements is greater than the number of participants that are certainly that will use or will not use it in the future. We think that the high number of participants that are in doubt is due to the inexperience in acceptance testing, which is in agreement with the background questionnaire where 100% of participants answered they had never seen it before.</div></div></div><div id="Sec16" class="Section1 RenderAsSection1"><h2 class="Heading"><span class="HeadingNumber">6 </span>Threats to the Validity</h2><div id="Par109" class="Para">Although we assigned different requirements to groups A and B in the same part of our experiment, we choose requirements that have similar business logic and complexity. However, the complexity of the requirements could affect results, mainly regarding time.</div><div id="Par110" class="Para">An expert in acceptance testing verified the artifacts produced by participants in part 1 and a business stakeholder verified the artifacts produced in part 2. These two individuals had an important role in our experimentation because they decided what is or not correct. However, we could carry out our experiment without these individuals in a different way:<div class="UnorderedList"><ul class="UnorderedListMarkDash"><li><div id="Par111" class="Para">using the acceptance tests specified by an expert as input for part 2, avoiding that some mistakes in the acceptance test scenarios created by other participants were unnoticed by the expert;</div></li><li><div id="Par112" class="Para">using automated acceptance tests, or even JUnit tests, to validate if the code implemented by the participants meets the user needs.</div></li></ul></div>
</div><div id="Par113" class="Para">However, we choose this approach to approximate our experiment to a real-world scenario, where there is a variation on the style of acceptance tests scenarios written, such as vocabulary, idioms, and level of details.</div><div id="Par114" class="Para">Another issue is the time sheets. It is very difficult to ensure that all participants are marking the time spent in each task. During the experiment, we checked if the forms have been filled correctly and asked the participants to fill out the forms very carefully. Finally, the small sample size may limit the capability of statistical tests. In this study, the time was compared using t-test, and for contingency tables, we used Fisher’s exact test.</div></div><div id="Sec17" class="Section1 RenderAsSection1"><h2 class="Heading"><span class="HeadingNumber">7 </span>Conclusion</h2><div id="Par115" class="Para">In this study, we have experimented to compare the applicability of acceptance tests, which were written using Fit tables and Gherkin language, as software requirements. The results show that there is no sufficient evidence to affirm that one technique is easier to use than another or one technique communicates software requirements better than another. Whereas, the comparison of effort regarding time to specify acceptance testing shows that the mean time to specify test scenarios using Gherkin Language is lower than using Fit tables.</div><div id="Par116" class="Para">Additionally, the questionnaire applied shows that participants had difficulty to specify and understand acceptance tests writing in both techniques. We assign this difficulty because neither of the participants had used Fit tables and Gherkin language before. Despite only a few participants answered that is easy to understand requirements expressed by acceptance tests, they have pointed out Gherkin language scenarios as easier to understand than Fit tables.</div><div id="Par117" class="Para">Finally, the number of participants who agreed with the possibility of using these acceptance testing techniques as software requirements in future projects is very similar to the numbers of those participants who disagree with this possibility. We assign this result to the participants’ inexperience in acceptance testing, which resulted in a poor impression about the application of these techniques in real-world projects. As future works, we intend to improve our experimental design to carry it out with others acceptance testing techniques and include other personas like non-technical users and software engineers.</div></div><div class="License LicenseSubType-cc-by"><a href="https://creativecommons.org/licenses/by/4.0"><img src="cc-by.png" alt="Creative Commons"/></a><div class="SimplePara">Open Access This chapter is licensed under the terms of the Creative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/), which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license and indicate if changes were made.
The images or other third party material in this book are included in the book's Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the book's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.</div></div><div class="Bibliography" id="Bib1"><div class="Heading">References</div><div class="BibliographyWrapper"><div class="Citation"><div class="CitationNumber">1.</div><div class="CitationContent" id="CR1">Sarmiento, E., Leite, J.C.S.P., Almentero, E.: C&amp;L: Generating model-based test cases from natural language requirements descriptions. In: 2014 IEEE 1st International Workshop on Requirements Engineering and Testing (RET), pp. 32–38 (2014)</div></div><div class="Citation"><div class="CitationNumber">2.</div><div class="CitationContent" id="CR2">Sommerville, I.: Software Engineering. 9th edn. Pearson Education, Boston (2015)</div></div><div class="Citation"><div class="CitationNumber">3.</div><div class="CitationContent" id="CR3">Torchiano, M., Ricca, F., Penta, M.D.: “Talking tests”: a preliminary experimental study on fit user acceptance tests. In: First International Symposium on Empirical Software Engineering and Measurement (ESEM 2007), pp. 464–466 (2007)</div></div><div class="Citation"><div class="CitationNumber">4.</div><div class="CitationContent" id="CR4">Ricca, F., Torchiano, M., Penta, M.D., Ceccato, M., Tonella, P.: Using acceptance tests as a support for clarifying requirements: A series of experiments. Inf. Softw. Technol. <span class="EmphasisTypeBold">51</span>, 270–283 (2009)<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a href="https://doi.org/10.1016/j.infsof.2008.01.007"><span><span>Crossref</span></span></a></span></span></div></div><div class="Citation"><div class="CitationNumber">5.</div><div class="CitationContent" id="CR5">Ricca, F., Torchiano, M., Penta, M.D., Ceccato, M., Tonella, P.: On the use of executable fit tables to support maintenance and evolution tasks. In: Third International ERCIM Symposium on Software Evolution, pp. 83–92 (2007)</div></div><div class="Citation"><div class="CitationNumber">6.</div><div class="CitationContent" id="CR6">Clerissi, D., Leotta, M., Reggio, G., Ricca, F.: A lightweight semi-automated acceptance test-driven development approach for web applications. In: Bozzon, A., Cudre-Maroux, P., Pautasso, C. (eds.) ICWE 2016. LNCS, vol. 9671, pp. 593–597. Springer, Cham (2016). <span class="ExternalRef"><a href="https://doi.org/10.1007/978-3-319-38791-8_55"><span class="RefSource">https://​doi.​org/​10.​1007/​978-3-319-38791-8_​55</span></a></span><span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a href="https://doi.org/10.1007/978-3-319-38791-8_55"><span><span>Crossref</span></span></a></span></span></div></div><div class="Citation"><div class="CitationNumber">7.</div><div class="CitationContent" id="CR7">Melnik, G., Maurer, F.: The practice of specifying requirements using executable acceptance tests in computer science courses. In: Companion to the 20th Annual ACM SIGPLAN Conference on Object-oriented Programming, Systems, Languages, and Applications, pp. 365–370. ACM, San Diego (2005)</div></div><div class="Citation"><div class="CitationNumber">8.</div><div class="CitationContent" id="CR8">Wanderley, F., Silva, A., Araújo, J.: Evaluation of BehaviorMap: a user-centered behavior language. In: 2015 IEEE 9th International Conference on Research Challenges in Information Science (RCIS), pp. 309–320 (2015)</div></div><div class="Citation"><div class="CitationNumber">9.</div><div class="CitationContent" id="CR9">Longo, D.H., Vilain P.: Creating user scenarios through user interaction diagrams by non-technical customers. In: Proceedings of the International Conference on Software Engineering and Knowledge Engineering, SEKE, pp. 330–335 (2015)</div></div><div class="Citation"><div class="CitationNumber">10.</div><div class="CitationContent" id="CR10">IEEE: IEEE Standard for Software Verification and Validation Plans. IEEE Std 1012-1986. IEEE (1986)</div></div><div class="Citation"><div class="CitationNumber">11.</div><div class="CitationContent" id="CR11">Mugridge, R., Cunningham, W.: Fit for Developing Software: Framework for Integrated Tests. Pearson Education, Upper Saddle River (2005)</div></div><div class="Citation"><div class="CitationNumber">12.</div><div class="CitationContent" id="CR12">Rose, S., Wynne, M., Hellesøy, A.: The Cucumber for Java Book: Behaviour-Driven Development for Testers and Developers. 1st edn. Pragmatic Bookshelf (2015)</div></div><div class="Citation"><div class="CitationNumber">13.</div><div class="CitationContent" id="CR13">Beck, K.: Test-Driven Development: By Example. Addison-Wesley Professional, Boston (2003)</div></div><div class="Citation"><div class="CitationNumber">14.</div><div class="CitationContent" id="CR14">Deng, C., Wilson, P., Maurer, F.: FitClipse: a fit-based eclipse plug-in for executable acceptance test driven development. In: Concas, G., Damiani, E., Scotto, M., Succi, G. (eds.) XP 2007. LNCS, vol. 4536, pp. 93–100. Springer, Heidelberg (2007). <span class="ExternalRef"><a href="https://doi.org/10.1007/978-3-540-73101-6_13"><span class="RefSource">https://​doi.​org/​10.​1007/​978-3-540-73101-6_​13</span></a></span><span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a href="https://doi.org/10.1007/978-3-540-73101-6_13"><span><span>Crossref</span></span></a></span></span></div></div><div class="Citation"><div class="CitationNumber">15.</div><div class="CitationContent" id="CR15">Wohlin, C., Runeson, P., Höst, M., Ohlsson, M.C., Regnell, B., Wesslén, A.: Experimentation in Software Engineering – An Introduction. Springer, Heidelberg (2012)<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a href="https://doi.org/10.1007/978-3-642-29044-2"><span><span>Crossref</span></span></a></span></span></div></div><div class="Citation"><div class="CitationNumber">16.</div><div class="CitationContent" id="CR16">Juristo, N., Moreno, A.: Basics of Software Engineering Experimentation. Kluwer Academic Publishers, Boston (2001)<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a href="https://doi.org/10.1007/978-1-4757-3304-4"><span><span>Crossref</span></span></a></span></span></div></div></div></div></div></body></html>
